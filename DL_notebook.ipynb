{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad517c19",
   "metadata": {},
   "source": [
    "# Group assignment DL - Accent classification\n",
    "\n",
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd92b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch\n",
    "# %pip install -r requirements.txt \n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "# import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1935806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: torch==2.4.1 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torchaudio) (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchaudio) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchaudio) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchaudio) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchaudio) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchaudio) (2025.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from jinja2->torch==2.4.1->torchaudio) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from sympy->torch==2.4.1->torchaudio) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd294bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cpu 2.4.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio\n",
    "print(torch.__version__, torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0c8f9",
   "metadata": {},
   "source": [
    "## FIX FIX FIXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d051fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.1\n",
      "Uninstalling torch-2.4.1:\n",
      "  Successfully uninstalled torch-2.4.1\n",
      "Found existing installation: torchvision 0.19.1\n",
      "Uninstalling torchvision-0.19.1:\n",
      "  Successfully uninstalled torchvision-0.19.1\n",
      "Found existing installation: torchaudio 2.4.1\n",
      "Uninstalling torchaudio-2.4.1:\n",
      "  Successfully uninstalled torchaudio-2.4.1\n",
      "Found existing installation: torch-directml 0.2.5.dev240914\n",
      "Uninstalling torch-directml-0.2.5.dev240914:\n",
      "  Successfully uninstalled torch-directml-0.2.5.dev240914\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Bram\\Documents\\GitHub\\Deep-Learning-Assignment\\.venv\\Lib\\site-packages\\~~rch'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Bram\\Documents\\GitHub\\Deep-Learning-Assignment\\.venv\\Lib\\site-packages\\~~rchaudio'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y torch torchvision torchaudio torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d73c868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-directml\n",
      "  Using cached torch_directml-0.2.5.dev240914-cp310-cp310-win_amd64.whl.metadata (6.2 kB)\n",
      "Collecting torch==2.4.1 (from torch-directml)\n",
      "  Using cached torch-2.4.1-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting torchvision==0.19.1 (from torch-directml)\n",
      "  Using cached torchvision-0.19.1-cp310-cp310-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torch-directml) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torch-directml) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torch-directml) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torch-directml) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torch-directml) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torch-directml) (2025.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torchvision==0.19.1->torch-directml) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torchvision==0.19.1->torch-directml) (11.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from jinja2->torch==2.4.1->torch-directml) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from sympy->torch==2.4.1->torch-directml) (1.3.0)\n",
      "Using cached torch_directml-0.2.5.dev240914-cp310-cp310-win_amd64.whl (9.0 MB)\n",
      "Using cached torch-2.4.1-cp310-cp310-win_amd64.whl (199.4 MB)\n",
      "Using cached torchvision-0.19.1-cp310-cp310-win_amd64.whl (1.3 MB)\n",
      "Installing collected packages: torch, torchvision, torch-directml\n",
      "\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   -------------------------- ------------- 2/3 [torch-directml]\n",
      "   ---------------------------------------- 3/3 [torch-directml]\n",
      "\n",
      "Successfully installed torch-2.4.1 torch-directml-0.2.5.dev240914 torchvision-0.19.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch-directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c5e7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (0.19.1)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.7.0-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: torch==2.4.1 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torchvision) (2.4.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (2025.3.2)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached torchaudio-2.6.0-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "  Using cached torchaudio-2.5.1-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "  Using cached torchaudio-2.5.0-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "  Using cached torchaudio-2.4.1-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from jinja2->torch==2.4.1->torchvision) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n",
      "Using cached torchaudio-2.4.1-cp310-cp310-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e1a1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "import torch, torch_directml, torchaudio\n",
    "device = torch_directml.device()\n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7daa5748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bram\\Documents\\GitHub\\Deep-Learning-Assignment\\.venv\\Scripts\\python.exe\n",
      "privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "# Are you really on your venv kernel?\n",
    "import sys; print(sys.executable)\n",
    "\n",
    "# Is your model/tensor on the DirectML device?\n",
    "x = torch.randn(10, device=device)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc157e77",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a562eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install https://files.pythonhosted.org/packages/…/torch_directml-0.2.5.dev240914-cp311-cp311-win_amd64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae6fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.5)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "ERROR: No matching distribution found for os\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3fe4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metadata(data_dir: str):\n",
    "    \"\"\"\n",
    "    Analyzing and collecting all metadata from the audio files (gender, male or female)\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith(\".wav\"):\n",
    "                path = os.path.join(root, fname)\n",
    "                accent = int(fname[0])          # '1'–'5'\n",
    "                gender = fname[1].lower()       # 'm' or 'f'\n",
    "                records.append({\"path\": path, \"accent\": accent, \"gender\": gender})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "class AccentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading, preprocessing, and feature-extracting audio.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata_df: pd.DataFrame,\n",
    "        approach: str = \"raw\",  # \"raw\" or \"mel\"\n",
    "        max_length: int = 16000 * 5,  # 5 seconds\n",
    "        sample_rate: int = 16000,\n",
    "        transform: torch.nn.Module = None,\n",
    "        target_transform = None\n",
    "    ):\n",
    "        self.df = metadata_df.reset_index(drop=True)\n",
    "        self.approach = approach\n",
    "        self.max_length = max_length\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # Silence trimming (VAD)\n",
    "        self.vad = torchaudio.transforms.Vad(sample_rate=sample_rate)\n",
    "\n",
    "        # Feature transforms (for 'mel' approach)\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mels=64,\n",
    "            n_fft=1024,\n",
    "            hop_length=512\n",
    "        )\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        waveform, sr = torchaudio.load(row[\"path\"])\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "        # Convert to mono\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Trim leading/trailing silence\n",
    "        waveform = self.vad(waveform)\n",
    "\n",
    "        # Pad or truncate to fixed length\n",
    "        length = waveform.size(1)\n",
    "        if length < self.max_length:\n",
    "            pad_amt = self.max_length - length\n",
    "            waveform = F.pad(waveform, (0, pad_amt))\n",
    "        else:\n",
    "            waveform = waveform[:, :self.max_length]\n",
    "\n",
    "        # Per-sample normalization\n",
    "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-9)\n",
    "\n",
    "        # Optional augmentations\n",
    "        if self.transform is not None:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        # Feature extraction\n",
    "        if self.approach == \"raw\":\n",
    "            features = waveform  # shape: [1, max_length]\n",
    "        elif self.approach == \"mel\":\n",
    "            mel_spec = self.mel_spectrogram(waveform)\n",
    "            features = self.db_transform(mel_spec)  # shape: [1, n_mels, time_steps]\n",
    "        else:\n",
    "            raise ValueError(\"approach must be 'raw' or 'mel'\")\n",
    "\n",
    "        label = row[\"accent\"]\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return features, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbe1cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "207d8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414345d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bram\\Documents\\GitHub\\Deep-Learning-Assignment\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb995b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_metadata( \"Train\" ) # Training dataframe based on accent & gender metadata\n",
    "raw_ds = AccentDataset(df, approach=\"raw\",  max_length=16000*5)\n",
    "mel_ds = AccentDataset(df, approach=\"mel\",  max_length=16000*5)\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 128\n",
    "raw_loader = DataLoader(raw_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "mel_loader = DataLoader(mel_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af46559",
   "metadata": {},
   "source": [
    "## 1.2a: Raw input signal -> analyze as 1D signal -> standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1b94f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\bram\\documents\\github\\deep-learning-assignment\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485089f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Splitting into train/val (80/20) with stratification on accent, so they each appear ~ in the same proportion in train/validation set\n",
    "val_fraction = 0.2\n",
    "df_train, df_val = train_test_split(\n",
    "    df,\n",
    "    test_size=val_fraction,\n",
    "    random_state=42,\n",
    "    stratify=df[\"accent\"]\n",
    ")\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val   = df_val.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "max_length = 16000 * 5  # 5 seconds\n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "\n",
    "# Raw waveform datasets and loaders\n",
    "train_ds = AccentDataset(\n",
    "    metadata_df=df_train,\n",
    "    approach=\"raw\",       # raw 1D signal\n",
    "    max_length=max_length,\n",
    ")\n",
    "val_ds   = AccentDataset(\n",
    "    metadata_df=df_val,\n",
    "    approach=\"raw\",\n",
    "    max_length=max_length,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a1bbaf",
   "metadata": {},
   "source": [
    "## Raw models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47082059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "class RawRNN1D(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2,\n",
    "                 num_classes=5, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=p_dropout,\n",
    "            nonlinearity='tanh'\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 1, T] -> [B, T, 1]\n",
    "        x = x.transpose(1, 2)\n",
    "        # rnn_out: [B, T, hidden]; h_n: [num_layers, B, hidden]\n",
    "        rnn_out, h_n = self.rnn(x)\n",
    "        # use last hidden state from top layer\n",
    "        last_h = h_n[-1]               # [B, hidden]\n",
    "        return self.classifier(last_h)\n",
    "\n",
    "\n",
    "class RawLSTM1D(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2,\n",
    "                 num_classes=5, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=p_dropout\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 1, T] -> [B, T, 1]\n",
    "        x = x.transpose(1, 2)\n",
    "        # lstm_out: [B, T, hidden]; (h_n, c_n)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # last hidden state\n",
    "        last_h = h_n[-1]               # [B, hidden]\n",
    "        return self.classifier(last_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb9ca5",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ce09e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DirectML on AMD GPU: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    import torch_directml\n",
    "    device = torch_directml.device()\n",
    "    print(\"Using DirectML on AMD GPU:\", device)\n",
    "except ImportError:\n",
    "    import torch\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"DirectML unavailable—using CPU:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5c340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48837ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "import torch_directml                # ← add this\n",
    "\n",
    "# --- 1. Define RNN and LSTM models ---\n",
    "\n",
    "class RawRNN1D(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2,\n",
    "                 num_classes=5, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=p_dropout,\n",
    "            nonlinearity='tanh'\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)        # [B,1,T] → [B,T,1]\n",
    "        rnn_out, h_n = self.rnn(x)\n",
    "        last_h = h_n[-1]             # [B, hidden]\n",
    "        return self.classifier(last_h)\n",
    "\n",
    "\n",
    "class RawLSTM1D(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2,\n",
    "                 num_classes=5, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=p_dropout\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)        # [B,1,T] → [B,T,1]\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        last_h = h_n[-1]             # [B, hidden]\n",
    "        return self.classifier(last_h)\n",
    "\n",
    "\n",
    "# --- 2. Instantiate, optimize, and train both models ---\n",
    "\n",
    "device = torch_directml.device()  # ← use DirectML on your AMD GPU\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_experiment(model_cls, train_loader, val_loader, **model_kwargs):\n",
    "    model = model_cls(**model_kwargs).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    for epoch in range(1, 11):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        total_loss, correct, count = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), (y-1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            count += x.size(0)\n",
    "        train_loss = total_loss / count\n",
    "        train_acc = correct / count\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        total_loss, correct, count = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), (y-1).to(device)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                count += x.size(0)\n",
    "        val_loss = total_loss / count\n",
    "        val_acc = correct / count\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"{model.__class__.__name__} \"\n",
    "              f\"Epoch {epoch:02d}  \"\n",
    "              f\"Train: {train_loss:.3f}, {train_acc:.3f} | \"\n",
    "              f\"Val: {val_loss:.3f}, {val_acc:.3f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# …then call run_experiment(RawRNN1D, …) and run_experiment(RawLSTM1D, …) as before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff8211",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f514d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DirectML on AMD GPU: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "import torch_directml\n",
    "\n",
    "import torch\n",
    "\n",
    "# Selecting device, as training was done on both AMD GPU and Apple Sillicon chip\n",
    "try:\n",
    "    import torch_directml\n",
    "    device = torch_directml.device()\n",
    "    print(\"Using DirectML on AMD GPU:\", device)\n",
    "\n",
    "except ImportError:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple MPS on Apple Silicon:\", device)\n",
    "    else:\n",
    "        # Fallback to CPU\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Falling back to CPU:\", device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def run_experiment(model_cls, train_loader, val_loader, **model_kwargs):\n",
    "    model = model_cls(**model_kwargs).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4    # L2 regularization\n",
    "    )\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    for epoch in range(1, 11):\n",
    "        # Train\n",
    "        model.train()\n",
    "        total_loss, correct, count = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), (y-1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds==y).sum().item()\n",
    "            count   += x.size(0)\n",
    "        train_loss = total_loss/count\n",
    "        train_acc  = correct/count\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        total_loss, correct, count = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), (y-1).to(device)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds==y).sum().item()\n",
    "                count   += x.size(0)\n",
    "        val_loss = total_loss/count\n",
    "        val_acc  = correct/count\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"{model.__class__.__name__} Epoch {epoch:02d}  \"\n",
    "              f\"Train: {train_loss:.3f}, {train_acc:.3f} | \"\n",
    "              f\"Val: {val_loss:.3f}, {val_acc:.3f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Run on your DataLoaders (raw_loader defined earlier)\n",
    "rnn_model, rnn_hist  = run_experiment(\n",
    "    RawRNN1D, train_loader=raw_loader, val_loader=val_loader,\n",
    "    input_size=1, hidden_size=128, num_layers=2, p_dropout=0.3, num_classes=5\n",
    ")\n",
    "lstm_model, lstm_hist = run_experiment(\n",
    "    RawLSTM1D, train_loader=raw_loader, val_loader=val_loader,\n",
    "    input_size=1, hidden_size=128, num_layers=2, p_dropout=0.3, num_classes=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439623e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
