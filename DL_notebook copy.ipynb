{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad517c19",
   "metadata": {},
   "source": [
    "# Group assignment DL V2 - Accent classification\n",
    "\n",
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd92b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch\n",
    "# %pip install -r requirements.txt # uncomment when all required libraries are defined\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3fe4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "\n",
    "def build_metadata(data_dir):\n",
    "    \"\"\"\n",
    "    Analyzing and collecting all metadata from the audio files (gender, male or female)\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith(\".wav\"):\n",
    "                path = os.path.join(root, fname)\n",
    "                accent = int(fname[0])          # '1'–'5'\n",
    "                gender = fname[1].lower()       # 'm' or 'f'\n",
    "                records.append({\"path\": path, \"accent\": accent, \"gender\": gender})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AccentDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata_df,\n",
    "        approach: str = \"raw\",            # \"raw\" or \"mel\"\n",
    "        sample_rate: int = 16000,\n",
    "        max_length_sec: float = 2.0,      # in seconds\n",
    "        n_mels: int = 64\n",
    "    ):\n",
    "        self.df = metadata_df.reset_index(drop=True)\n",
    "        self.approach = approach\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = int(sample_rate * max_length_sec)\n",
    "\n",
    "        # Only for mel\n",
    "        if self.approach == \"mel\":\n",
    "            self.mel_spec = T.MelSpectrogram(\n",
    "                sample_rate=sample_rate,\n",
    "                n_fft=400,\n",
    "                hop_length=160,\n",
    "                n_mels=n_mels\n",
    "            )\n",
    "            self.to_db = T.AmplitudeToDB()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        waveform, sr = torchaudio.load(row[\"path\"])  # [1, L]\n",
    "        # resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = T.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "        # pad or trim to exact length\n",
    "        L = waveform.size(1)\n",
    "        if L < self.max_length:\n",
    "            pad_amt = self.max_length - L\n",
    "            waveform = F.pad(waveform, (0, pad_amt))\n",
    "        else:\n",
    "            waveform = waveform[:, : self.max_length]\n",
    "\n",
    "        # now waveform is [1, max_length]\n",
    "        # normalize\n",
    "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-6)\n",
    "\n",
    "        if self.approach == \"raw\":\n",
    "            x = waveform         # shape [1, max_length]\n",
    "        else:\n",
    "            m = self.mel_spec(waveform)  # [1, n_mels, time_frames]\n",
    "            x = self.to_db(m)            # log‐scale\n",
    "\n",
    "        # label as 0–4\n",
    "        y = torch.tensor(row[\"accent\"] - 1, dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbe1cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff826f7c",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40ff4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) Split\n",
    "df_train, df_val = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[\"accent\"]\n",
    ")\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val   = df_val.reset_index(drop=True)\n",
    "\n",
    "# 2) Datasets\n",
    "raw_train_ds = AccentDataset(df_train, approach=\"raw\", max_length_sec=2.0)\n",
    "raw_val_ds   = AccentDataset(df_val,   approach=\"raw\", max_length_sec=2.0)\n",
    "mel_train_ds = AccentDataset(df_train, approach=\"mel\", max_length_sec=2.0)\n",
    "mel_val_ds   = AccentDataset(df_val,   approach=\"mel\", max_length_sec=2.0)\n",
    "\n",
    "# 3) Loaders\n",
    "batch_size = 32\n",
    "train_loader_raw = DataLoader(raw_train_ds, batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader_raw   = DataLoader(raw_val_ds,   batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "train_loader_mel = DataLoader(mel_train_ds, batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader_mel   = DataLoader(mel_val_ds,   batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11977809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 20, 80, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_raw), len(val_loader_raw), len(train_loader_mel), len(val_loader_mel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45dab78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn as nn\n",
    "\n",
    "class RawCNN1D(nn.Module):\n",
    "    def __init__(self, num_classes=5, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Conv + aggressive pooling to cut 80k → 5k → 1\n",
    "            nn.Conv1d(1, 32, kernel_size=9, padding=4), nn.ReLU(),\n",
    "            nn.MaxPool1d(4),  # 80k→20k\n",
    "            nn.Conv1d(32, 64, kernel_size=9, padding=4), nn.ReLU(),\n",
    "            nn.MaxPool1d(4),  # 20k→5k\n",
    "            nn.Conv1d(64, 128, kernel_size=9, padding=4), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),  # 5k→1\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p_dropout)\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,1,T]\n",
    "        feat = self.net(x)       # [B,128]\n",
    "        return self.classifier(feat)\n",
    "\n",
    "\n",
    "class MelCNN2D(nn.Module):\n",
    "    def __init__(self, num_classes=5, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # x: [B,1,n_mels,time]\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # n_mels/2 × time/2\n",
    "            nn.Conv2d(32,64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # n_mels/4 × time/4\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)), # collapse to [B,128,1,1]\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p_dropout)\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.net(x)  # [B,128]\n",
    "        return self.classifier(feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f90eb89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # in Jupyter/VSCode notebooks\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_experiment(model_cls, train_loader, val_loader, epochs=10, **model_kwargs):\n",
    "    model = model_cls(**model_kwargs).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    print(f\"\\nTraining {model_cls.__name__} for {epochs} epochs on {device}\\n\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # --- Training (with tqdm progress bar) ---\n",
    "        model.train()\n",
    "        total_loss = correct = count = 0\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [TRAIN]\", leave=False)\n",
    "        for batch_idx, (x, y) in enumerate(train_bar, 1):\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = (y - 1).to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            count   += x.size(0)\n",
    "\n",
    "            # update bar description with latest batch loss\n",
    "            train_bar.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "        train_loss = total_loss / count\n",
    "        train_acc  = correct    / count\n",
    "\n",
    "        # --- Validation (with tqdm) ---\n",
    "        model.eval()\n",
    "        total_loss = correct = count = 0\n",
    "        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [  VAL ]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_bar:\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                y = (y - 1).to(device, non_blocking=True)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                count   += x.size(0)\n",
    "\n",
    "                val_bar.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "        val_loss = total_loss / count\n",
    "        val_acc  = correct    / count\n",
    "\n",
    "        # record history\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"[{model_cls.__name__}] Epoch {epoch:02d}/{epochs}  \"\n",
    "              f\"Train: loss={train_loss:.4f}, acc={train_acc:.4f}  |  \"\n",
    "              f\"Val: loss={val_loss:.4f}, acc={val_acc:.4f}  |  \"\n",
    "              f\"Time: {epoch_time:.1f}s\")\n",
    "\n",
    "    print(\"\\nTraining complete.\\n\")\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22bc138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using Apple MPS on Apple Silicon: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Cross-platform device selection\n",
    "try:\n",
    "    import torch_directml\n",
    "    device = torch_directml.device()\n",
    "    print(\"🚀 Using DirectML on AMD GPU:\", device)\n",
    "except ImportError:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"🚀 Using Apple MPS on Apple Silicon:\", device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"⚠️  Falling back to CPU:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd447c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RawCNN1D for 5 epochs on mps\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace7ef264a374f6eb44383c09e4d81a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 [TRAIN]:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/bramdewaal/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bramdewaal/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'AccentDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "# Raw 1D CNN on waveform\n",
    "raw_model, raw_hist = run_experiment(\n",
    "    RawCNN1D,\n",
    "    train_loader=train_loader_raw,\n",
    "    val_loader=val_loader_raw,\n",
    "    epochs=5,\n",
    "    num_classes=5,\n",
    "    p_dropout=0.3\n",
    ")\n",
    "\n",
    "# # 2D CNN on Mel spectrograms\n",
    "# mel_model, mel_hist = run_experiment(\n",
    "#     MelCNN2D,\n",
    "#     train_loader=train_loader_mel,\n",
    "#     val_loader=val_loader_mel,\n",
    "#     epochs=5,\n",
    "#     num_classes=5,\n",
    "#     p_dropout=0.3\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9086d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(raw_history, model_name=\"RawCNN1D\")\n",
    "plot_training_curves(mel_history, model_name=\"MelCNN2D\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f25d10",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af46559",
   "metadata": {},
   "source": [
    "## 1.2a: Raw input signal -> analyze as 1D signal -> standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "485089f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Splitting into train/val (80/20) with stratification on accent, so they each appear ~ in the same proportion in train/validation set\n",
    "val_fraction = 0.2\n",
    "df_train, df_val = train_test_split(\n",
    "    df,\n",
    "    test_size=val_fraction,\n",
    "    random_state=42,\n",
    "    stratify=df[\"accent\"]\n",
    ")\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val   = df_val.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "max_length = 16000 * 5  # 5 seconds\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "# Raw waveform datasets and loaders\n",
    "train_ds = AccentDataset(\n",
    "    metadata_df=df_train,\n",
    "    approach=\"raw\",       # raw 1D signal\n",
    "    max_length=max_length,\n",
    ")\n",
    "val_ds   = AccentDataset(\n",
    "    metadata_df=df_val,\n",
    "    approach=\"raw\",\n",
    "    max_length=max_length,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a1bbaf",
   "metadata": {},
   "source": [
    "## Raw models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47082059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from sklearn.metrics import classification_report\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class RawCNN1D(nn.Module):\n",
    "#     def __init__(self, num_classes=5, p_dropout=0.3):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             # Conv + aggressive pooling to cut 80k → 5k → 1\n",
    "#             nn.Conv1d(1, 32, kernel_size=9, padding=4), nn.ReLU(),\n",
    "#             nn.MaxPool1d(4),  # 80k→20k\n",
    "#             nn.Conv1d(32, 64, kernel_size=9, padding=4), nn.ReLU(),\n",
    "#             nn.MaxPool1d(4),  # 20k→5k\n",
    "#             nn.Conv1d(64, 128, kernel_size=9, padding=4), nn.ReLU(),\n",
    "#             nn.AdaptiveAvgPool1d(1),  # 5k→1\n",
    "#             nn.Flatten(),\n",
    "#             nn.Dropout(p_dropout)\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: [B,1,T]\n",
    "#         feat = self.net(x)       # [B,128]\n",
    "#         return self.classifier(feat)\n",
    "\n",
    "\n",
    "# class MelCNN2D(nn.Module):\n",
    "#     def __init__(self, num_classes=5, p_dropout=0.3):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             # x: [B,1,n_mels,time]\n",
    "#             nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),  # n_mels/2 × time/2\n",
    "#             nn.Conv2d(32,64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),  # n_mels/4 × time/4\n",
    "#             nn.Conv2d(64,128,kernel_size=3,padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "#             nn.AdaptiveAvgPool2d((1,1)), # collapse to [B,128,1,1]\n",
    "#             nn.Flatten(),\n",
    "#             nn.Dropout(p_dropout)\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         feat = self.net(x)  # [B,128]\n",
    "#         return self.classifier(feat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb9ca5",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48837ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "            # ← add this\n",
    "\n",
    "# --- 1. Define RNN and LSTM models ---\n",
    "\n",
    "class RawRNN1D(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2,\n",
    "                 num_classes=5, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=p_dropout,\n",
    "            nonlinearity='tanh'\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)        # [B,1,T] → [B,T,1]\n",
    "        rnn_out, h_n = self.rnn(x)\n",
    "        last_h = h_n[-1]             # [B, hidden]\n",
    "        return self.classifier(last_h)\n",
    "\n",
    "\n",
    "class RawLSTM1D(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2,\n",
    "                 num_classes=5, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=p_dropout\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)        # [B,1,T] → [B,T,1]\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        last_h = h_n[-1]             # [B, hidden]\n",
    "        return self.classifier(last_h)\n",
    "\n",
    "\n",
    "# --- 2. Instantiate, optimize, and train both models ---\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_experiment(model_cls, train_loader, val_loader, **model_kwargs):\n",
    "    model = model_cls(**model_kwargs).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    for epoch in range(1, 11):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        total_loss, correct, count = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), (y-1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            count += x.size(0)\n",
    "        train_loss = total_loss / count\n",
    "        train_acc = correct / count\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        total_loss, correct, count = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), (y-1).to(device)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                count += x.size(0)\n",
    "        val_loss = total_loss / count\n",
    "        val_acc = correct / count\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"{model.__class__.__name__} \"\n",
    "              f\"Epoch {epoch:02d}  \"\n",
    "              f\"Train: {train_loss:.3f}, {train_acc:.3f} | \"\n",
    "              f\"Val: {val_loss:.3f}, {val_acc:.3f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# …then call run_experiment(RawRNN1D, …) and run_experiment(RawLSTM1D, …) as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c0f514d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bramdewaal/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/bramdewaal/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bramdewaal/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'AccentDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Run on your DataLoaders (raw_loader defined earlier)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m rnn_model, rnn_hist  \u001b[38;5;241m=\u001b[39m run_experiment(\n\u001b[1;32m     59\u001b[0m     RawRNN1D, train_loader\u001b[38;5;241m=\u001b[39mraw_loader, val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m     60\u001b[0m     input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, p_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     62\u001b[0m lstm_model, lstm_hist \u001b[38;5;241m=\u001b[39m run_experiment(\n\u001b[1;32m     63\u001b[0m     RawLSTM1D, train_loader\u001b[38;5;241m=\u001b[39mraw_loader, val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m     64\u001b[0m     input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, p_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     65\u001b[0m )\n",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(model_cls, train_loader, val_loader, **model_kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     16\u001b[0m total_loss, correct, count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     18\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), (y\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_experiment(model_cls, train_loader, val_loader, **model_kwargs):\n",
    "    model = model_cls(**model_kwargs).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4    # L2 regularization\n",
    "    )\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    for epoch in range(1, 11):\n",
    "        # Train\n",
    "        model.train()\n",
    "        total_loss, correct, count = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), (y-1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds==y).sum().item()\n",
    "            count   += x.size(0)\n",
    "        train_loss = total_loss/count\n",
    "        train_acc  = correct/count\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        total_loss, correct, count = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), (y-1).to(device)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds==y).sum().item()\n",
    "                count   += x.size(0)\n",
    "        val_loss = total_loss/count\n",
    "        val_acc  = correct/count\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"{model.__class__.__name__} Epoch {epoch:02d}  \"\n",
    "              f\"Train: {train_loss:.3f}, {train_acc:.3f} | \"\n",
    "              f\"Val: {val_loss:.3f}, {val_acc:.3f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Run on your DataLoaders (raw_loader defined earlier)\n",
    "rnn_model, rnn_hist  = run_experiment(\n",
    "    RawRNN1D, train_loader=raw_loader, val_loader=val_loader,\n",
    "    input_size=1, hidden_size=128, num_layers=2, p_dropout=0.3, num_classes=5\n",
    ")\n",
    "lstm_model, lstm_hist = run_experiment(\n",
    "    RawLSTM1D, train_loader=raw_loader, val_loader=val_loader,\n",
    "    input_size=1, hidden_size=128, num_layers=2, p_dropout=0.3, num_classes=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439623e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
