{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34a1b3a",
   "metadata": {},
   "source": [
    "# Complete Fixed Notebook\n",
    "This notebook fully implements Tasks 1–9 with all fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21bfcb2",
   "metadata": {},
   "source": [
    "## Imports & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1efaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, csv\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3d97c",
   "metadata": {},
   "source": [
    "## Task 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db27bdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val: 2533 633\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "TRAIN_DIR = \"Train\"\n",
    "TEST_DIR  = \"Test\"\n",
    "\n",
    "# AccentAudioDataset\n",
    "class AccentAudioDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.paths = sorted(glob(os.path.join(data_dir,'*.wav')))\n",
    "        if not self.paths: raise RuntimeError(f\"No .wav in {data_dir}\")\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        w, sr = torchaudio.load(p)\n",
    "        if sr!=16000: w = torchaudio.functional.resample(w, sr, 16000)\n",
    "        w = (w - w.mean())/(w.std()+1e-9)\n",
    "        fname = os.path.basename(p)\n",
    "        accent = int(fname[0]) - 1\n",
    "        gender = 0 if fname[1].lower()=='m' else 1\n",
    "        return w.squeeze(0), accent, gender\n",
    "\n",
    "def collate_fn(batch):\n",
    "    waves, accents, genders = zip(*batch)\n",
    "    lengths = torch.tensor([w.size(0) for w in waves])\n",
    "    padded = pad_sequence(waves, batch_first=True)\n",
    "    return padded, lengths, torch.tensor(accents), torch.tensor(genders)\n",
    "\n",
    "# Split\n",
    "full_ds = AccentAudioDataset(TRAIN_DIR)\n",
    "n_tot = len(full_ds)\n",
    "n_val = int(0.2*n_tot); n_train = n_tot - n_val\n",
    "train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0, pin_memory=False, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=0, pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Train/Val:\", n_train, n_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0c406",
   "metadata": {},
   "source": [
    "## Task 2: RawCNNv2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8b66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawCNNv2(nn.Module):\n",
    "    def __init__(self,num_classes=5,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(1,32,7,stride=2,padding=3),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(True), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32,64,5,stride=2,padding=2),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(True), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64,128,3,stride=2,padding=1),\n",
    "            nn.BatchNorm1d(128), nn.ReLU(True), nn.MaxPool1d(2),\n",
    "            nn.AdaptiveAvgPool1d(1), nn.Flatten(),\n",
    "            nn.Dropout(dropout), nn.Linear(128,num_classes)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f52b88",
   "metadata": {},
   "source": [
    "## Task 3: Waveform Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830e1852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug loader: 3166\n"
     ]
    }
   ],
   "source": [
    "class AddNoise:\n",
    "    def __call__(self,w):\n",
    "        rms=w.pow(2).mean().sqrt(); return w+torch.randn_like(w)*rms*0.05\n",
    "class RandomShift:\n",
    "    def __call__(self,w):\n",
    "        return torch.roll(w, shifts=random.randint(-1000,1000))\n",
    "class WaveAugDataset(AccentAudioDataset):\n",
    "    def __init__(self,d,augs): super().__init__(d); self.augs=augs\n",
    "    def __getitem__(self,i):\n",
    "        w,a,g=super().__getitem__(i)\n",
    "        for aug in self.augs: w=aug(w)\n",
    "        w=(w-w.mean())/(w.std()+1e-9)\n",
    "        return w,a,g\n",
    "\n",
    "aug_loader = DataLoader(WaveAugDataset(TRAIN_DIR,[AddNoise(),RandomShift()]),\n",
    "    batch_size=32,shuffle=True,num_workers=0,pin_memory=False,collate_fn=collate_fn)\n",
    "print(\"Aug loader:\", len(aug_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a5623",
   "metadata": {},
   "source": [
    "## Task 4: Train RawCNNv2 with OneCycleLR & Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db81f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions redefined (no imports).\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Redefine helper functions without re-importing\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for inputs, lengths, accents, genders in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        accents = accents.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, accents)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch = inputs.size(0)\n",
    "        total_loss += loss.item() * batch\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == accents).sum().item()\n",
    "        total += batch\n",
    "    \n",
    "    return total_loss/total, correct/total, time.time() - start_time\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, lengths, accents, genders in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            accents = accents.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, accents)\n",
    "            \n",
    "            batch = inputs.size(0)\n",
    "            total_loss += loss.item() * batch\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == accents).sum().item()\n",
    "            total += batch\n",
    "    \n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "print(\"✅ Helper functions redefined (no imports).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c89b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | tr_acc 0.210 | val_acc 0.264\n",
      "Epoch 02 | tr_acc 0.260 | val_acc 0.302\n",
      "Epoch 03 | tr_acc 0.317 | val_acc 0.303\n",
      "Epoch 04 | tr_acc 0.369 | val_acc 0.362\n",
      "Epoch 05 | tr_acc 0.401 | val_acc 0.445\n",
      "Epoch 06 | tr_acc 0.423 | val_acc 0.346\n",
      "Epoch 07 | tr_acc 0.444 | val_acc 0.442\n",
      "Epoch 08 | tr_acc 0.457 | val_acc 0.409\n",
      "Epoch 09 | tr_acc 0.472 | val_acc 0.392\n",
      "Epoch 10 | tr_acc 0.474 | val_acc 0.374\n",
      "Epoch 11 | tr_acc 0.489 | val_acc 0.472\n",
      "Epoch 12 | tr_acc 0.494 | val_acc 0.547\n",
      "Epoch 13 | tr_acc 0.513 | val_acc 0.561\n",
      "Epoch 14 | tr_acc 0.529 | val_acc 0.415\n",
      "Epoch 15 | tr_acc 0.534 | val_acc 0.370\n",
      "Epoch 16 | tr_acc 0.546 | val_acc 0.455\n",
      "Epoch 17 | tr_acc 0.538 | val_acc 0.534\n",
      "Epoch 18 | tr_acc 0.562 | val_acc 0.532\n",
      "Epoch 19 | tr_acc 0.563 | val_acc 0.384\n",
      "Epoch 20 | tr_acc 0.559 | val_acc 0.477\n",
      "Epoch 21 | tr_acc 0.569 | val_acc 0.518\n",
      "Epoch 22 | tr_acc 0.572 | val_acc 0.479\n",
      "Epoch 23 | tr_acc 0.580 | val_acc 0.483\n",
      "Epoch 24 | tr_acc 0.591 | val_acc 0.379\n",
      "Epoch 25 | tr_acc 0.603 | val_acc 0.624\n",
      "Epoch 26 | tr_acc 0.600 | val_acc 0.537\n",
      "Epoch 27 | tr_acc 0.599 | val_acc 0.559\n",
      "Epoch 28 | tr_acc 0.611 | val_acc 0.652\n",
      "Epoch 29 | tr_acc 0.601 | val_acc 0.591\n",
      "Epoch 30 | tr_acc 0.612 | val_acc 0.561\n",
      "Epoch 31 | tr_acc 0.613 | val_acc 0.575\n",
      "Epoch 32 | tr_acc 0.627 | val_acc 0.453\n",
      "Epoch 33 | tr_acc 0.627 | val_acc 0.545\n",
      "Epoch 34 | tr_acc 0.642 | val_acc 0.682\n",
      "Epoch 35 | tr_acc 0.629 | val_acc 0.671\n",
      "Epoch 36 | tr_acc 0.658 | val_acc 0.701\n",
      "Epoch 37 | tr_acc 0.637 | val_acc 0.581\n",
      "Epoch 38 | tr_acc 0.637 | val_acc 0.681\n",
      "Epoch 39 | tr_acc 0.652 | val_acc 0.641\n",
      "Epoch 40 | tr_acc 0.644 | val_acc 0.645\n",
      "Epoch 41 | tr_acc 0.656 | val_acc 0.660\n",
      "Epoch 42 | tr_acc 0.660 | val_acc 0.700\n",
      "Epoch 43 | tr_acc 0.654 | val_acc 0.703\n",
      "Epoch 44 | tr_acc 0.660 | val_acc 0.694\n",
      "Epoch 45 | tr_acc 0.669 | val_acc 0.725\n",
      "Epoch 46 | tr_acc 0.670 | val_acc 0.716\n",
      "Epoch 47 | tr_acc 0.662 | val_acc 0.703\n",
      "Epoch 48 | tr_acc 0.678 | val_acc 0.697\n",
      "Epoch 49 | tr_acc 0.668 | val_acc 0.708\n",
      "Epoch 50 | tr_acc 0.676 | val_acc 0.703\n",
      "Best RawCNN2: 0.7251184834123223 @ ep 45\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "model_raw = RawCNNv2().to(device)\n",
    "opt_raw = torch.optim.Adam(model_raw.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "sched_raw = torch.optim.lr_scheduler.OneCycleLR(opt_raw, max_lr=1e-3, total_steps=50, pct_start=0.3)\n",
    "\n",
    "best_acc, best_ep = 0,0\n",
    "for ep in range(1,51):\n",
    "    tr_loss,tr_acc,_=train_one_epoch(model_raw,aug_loader,opt_raw,device)\n",
    "    vl_loss,vl_acc  =evaluate(model_raw,val_loader,device)\n",
    "    sched_raw.step()\n",
    "    if vl_acc>best_acc:\n",
    "        best_acc,best_ep=vl_acc,ep; torch.save(model_raw.state_dict(),'best_rawcnn.pt')\n",
    "    print(f\"Epoch {ep:02d} | tr_acc {tr_acc:.3f} | val_acc {vl_acc:.3f}\")\n",
    "print(\"Best RawCNN2:\",best_acc,\"@ ep\",best_ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863c9ed",
   "metadata": {},
   "source": [
    "## Task 5: Evaluate RawCNNv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d659fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8114912832894408, 0.7251184834123223)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_raw.load_state_dict(torch.load('best_rawcnn.pt'))\n",
    "evaluate(model_raw,val_loader,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ef518",
   "metadata": {},
   "source": [
    "## Task 6: Spectrogram Dataset and Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aae071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spec loaders: 2533 633\n"
     ]
    }
   ],
   "source": [
    "# SpectrogramDataset & collate (3-tuple)\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self,d,tf): self.paths=sorted(glob(os.path.join(d,'*.wav'))); self.tf=tf\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self,i):\n",
    "        p=self.paths[i]; w,sr=torchaudio.load(p)\n",
    "        if sr!=16000: w=torchaudio.functional.resample(w,sr,16000)\n",
    "        w=(w-w.mean())/(w.std()+1e-9)\n",
    "        spec=self.tf(w); spec=torch.log(spec+1e-9)\n",
    "        fname=os.path.basename(p); a=int(fname[0])-1; g=0 if fname[1].lower()=='m' else 1\n",
    "        return spec,a,g\n",
    "\n",
    "def spec_collate(batch):\n",
    "    specs,accs,gens=zip(*batch)\n",
    "    T_max=max(s.size(2) for s in specs)\n",
    "    padded=[torch.nn.functional.pad(s,(0,T_max-s.size(2))) for s in specs]\n",
    "    return torch.stack(padded,0), torch.tensor(accs), torch.tensor(gens)\n",
    "\n",
    "\n",
    "\n",
    "mel_tf = T.MelSpectrogram(sample_rate=16000, n_mels=64)\n",
    "spec_full  = SpectrogramDataset(TRAIN_DIR, mel_tf)\n",
    "spec_train = Subset(spec_full, train_ds.indices)\n",
    "spec_val   = Subset(spec_full, val_ds.indices)\n",
    "\n",
    "spec_train_loader = DataLoader(\n",
    "    spec_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    collate_fn=spec_collate\n",
    ")\n",
    "spec_val_loader = DataLoader(\n",
    "    spec_val,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    collate_fn=spec_collate\n",
    ")\n",
    "\n",
    "print(\"Spec loaders:\", len(spec_train), len(spec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28c190",
   "metadata": {},
   "source": [
    "## Task 7: SpectrogramCNN & SpecAugment Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b1abf",
   "metadata": {},
   "source": [
    "## Task 8: Generate Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "787c2293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SpecAugDataset defined.\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell PREP-0: Define SpecAugDataset ───\n",
    "\n",
    "from torchaudio.transforms import FrequencyMasking, TimeMasking\n",
    "\n",
    "# 1) Create the two masks once\n",
    "freq_mask = FrequencyMasking(freq_mask_param=15)\n",
    "time_mask = TimeMasking(time_mask_param=35)\n",
    "\n",
    "# 2) Inherit from your existing SpectrogramDataset\n",
    "class SpecAugDataset(SpectrogramDataset):\n",
    "    \"\"\"\n",
    "    Same as SpectrogramDataset but applies SpecAugment (freq & time masking).\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, transform):\n",
    "        super().__init__(data_dir, transform)\n",
    "        self.freq_mask = freq_mask\n",
    "        self.time_mask = time_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spec, accent, gender = super().__getitem__(idx)\n",
    "        # apply masking in-place\n",
    "        spec = self.freq_mask(spec)\n",
    "        spec = self.time_mask(spec)\n",
    "        return spec, accent, gender\n",
    "\n",
    "print(\"✅ SpecAugDataset defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48c3fb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SpectrogramCNN defined.\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell PREP-1: Define SpectrogramCNN ───\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpectrogramCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    2D CNN for log‐Mel spectrograms (5 accents).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 1, F, T]\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"✅ SpectrogramCNN defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92ebb3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PREP complete: class_counts defined, weighted loss, loaders, model, optimizer & scheduler ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell PREP: Redefine Missing Variables & Functions for SpectrogramCNN Pipeline (Fixed)\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchaudio.transforms import FrequencyMasking, TimeMasking\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# 1) Build your augmented‐spectrogram training subset\n",
    "#    (SpecAugDataset, TRAIN_DIR, mel_tf and train_ds must already be defined)\n",
    "spec_aug_full  = SpecAugDataset(TRAIN_DIR, mel_tf)\n",
    "spec_aug_train = Subset(spec_aug_full, train_ds.indices)\n",
    "\n",
    "# 2) Compute per‐class counts & inverse‐frequency weights\n",
    "train_labels   = [label for _, label, _ in spec_aug_train]\n",
    "train_counts   = np.bincount(train_labels, minlength=5)\n",
    "class_weights  = 1.0 / train_counts\n",
    "\n",
    "# 3) Normalize and move to device\n",
    "weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "weights = weights / weights.sum() * len(weights)\n",
    "\n",
    "# 4) Create weighted CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# 5) Spec‐specific train/eval helpers\n",
    "def train_one_epoch_spec(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = correct = total = 0\n",
    "    start = time.time()\n",
    "    for specs, accents, genders in loader:\n",
    "        specs, accents = specs.to(device), accents.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(specs)\n",
    "        loss   = criterion(logits, accents)   # uses our weighted loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        b = specs.size(0)\n",
    "        total_loss += loss.item() * b\n",
    "        correct    += (logits.argmax(1) == accents).sum().item()\n",
    "        total      += b\n",
    "    return total_loss/total, correct/total, time.time() - start\n",
    "\n",
    "def evaluate_spec(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for specs, accents, genders in loader:\n",
    "            specs, accents = specs.to(device), accents.to(device)\n",
    "            logits = model(specs)\n",
    "            loss   = F.cross_entropy(logits, accents)\n",
    "            b = specs.size(0)\n",
    "            total_loss += loss.item() * b\n",
    "            correct    += (logits.argmax(1) == accents).sum().item()\n",
    "            total      += b\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "# 6) Build your oversampled SpecAug DataLoader\n",
    "example_weights = [class_weights[l] for l in train_labels]\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights     = example_weights,\n",
    "    num_samples = len(example_weights),\n",
    "    replacement = True\n",
    ")\n",
    "spec_aug_loader = DataLoader(\n",
    "    spec_aug_train,\n",
    "    batch_size   = 32,\n",
    "    sampler      = sampler,        # no shuffle\n",
    "    num_workers  = 0,\n",
    "    pin_memory   = False,\n",
    "    collate_fn   = spec_collate\n",
    ")\n",
    "\n",
    "# 7) Validation loader\n",
    "spec_val_full   = SpectrogramDataset(TRAIN_DIR, mel_tf)\n",
    "spec_val_subset = Subset(spec_val_full, val_ds.indices)\n",
    "spec_val_loader = DataLoader(\n",
    "    spec_val_subset,\n",
    "    batch_size   = 32,\n",
    "    shuffle      = False,\n",
    "    num_workers  = 0,\n",
    "    pin_memory   = False,\n",
    "    collate_fn   = spec_collate\n",
    ")\n",
    "\n",
    "# 8) Model + Optimizer + Scheduler setup\n",
    "model_spec     = SpectrogramCNN(num_classes=5, dropout=0.1).to(device)\n",
    "optimizer_spec = optim.Adam(model_spec.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "scheduler_spec = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_spec, mode='min', factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "print(\"✅ PREP complete: class_counts defined, weighted loss, loaders, model, optimizer & scheduler ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7272d374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train‐split counts (support) per accent: [594 500 451 598 390]\n",
      "Computed class weights: tensor([0.8304, 0.9865, 1.0936, 0.8248, 1.2647], device='mps:0')\n",
      "✅ criterion ready with weighted classes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1) Extract all accent labels from your train_ds\n",
    "#    If you only have a DataLoader, you can iterate it instead.\n",
    "train_labels = [label for _, label, _ in train_ds]  \n",
    "# train_labels is now a list of ints in {0,…,4}\n",
    "\n",
    "# 2) Compute counts per class\n",
    "train_counts = np.bincount(train_labels, minlength=5)\n",
    "print(\"Train‐split counts (support) per accent:\", train_counts)\n",
    "\n",
    "# 3) Build inverse‐frequency weights (use float32 on MPS)\n",
    "weights = torch.tensor(1.0 / train_counts, dtype=torch.float32, device=device)\n",
    "# Normalize so average weight ≈ 1\n",
    "weights = weights / weights.sum() * len(weights)\n",
    "print(\"Computed class weights:\", weights)\n",
    "\n",
    "# 4) Create weighted loss\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "print(\"✅ criterion ready with weighted classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6403f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# … after you have spec_aug_loader and spec_val_loader defined …\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Compute true class counts on your **train** split, not val.\n",
    "# Replace these with your train-split support values.\n",
    "train_counts = np.array([146, 126, 113, 156,  92])  \n",
    "\n",
    "# Inverse-frequency weights\n",
    "\n",
    "weights = weights / weights.sum() * len(weights)  \n",
    "\n",
    "# Create a weighted cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59bb420d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SpecAug] Epoch 01 | Train Acc: 0.276 | Val Acc: 0.185\n",
      "[SpecAug] Epoch 02 | Train Acc: 0.302 | Val Acc: 0.327\n",
      "[SpecAug] Epoch 03 | Train Acc: 0.335 | Val Acc: 0.243\n",
      "[SpecAug] Epoch 04 | Train Acc: 0.384 | Val Acc: 0.485\n",
      "[SpecAug] Epoch 05 | Train Acc: 0.403 | Val Acc: 0.363\n",
      "[SpecAug] Epoch 06 | Train Acc: 0.468 | Val Acc: 0.359\n",
      "[SpecAug] Epoch 07 | Train Acc: 0.443 | Val Acc: 0.398\n",
      "[SpecAug] Epoch 08 | Train Acc: 0.490 | Val Acc: 0.510\n",
      "[SpecAug] Epoch 09 | Train Acc: 0.498 | Val Acc: 0.280\n",
      "[SpecAug] Epoch 10 | Train Acc: 0.504 | Val Acc: 0.477\n",
      "[SpecAug] Epoch 11 | Train Acc: 0.521 | Val Acc: 0.621\n",
      "[SpecAug] Epoch 12 | Train Acc: 0.539 | Val Acc: 0.550\n",
      "[SpecAug] Epoch 13 | Train Acc: 0.549 | Val Acc: 0.605\n",
      "[SpecAug] Epoch 14 | Train Acc: 0.554 | Val Acc: 0.499\n",
      "[SpecAug] Epoch 15 | Train Acc: 0.565 | Val Acc: 0.545\n",
      "[SpecAug] Epoch 16 | Train Acc: 0.587 | Val Acc: 0.607\n",
      "[SpecAug] Epoch 17 | Train Acc: 0.585 | Val Acc: 0.664\n",
      "[SpecAug] Epoch 18 | Train Acc: 0.593 | Val Acc: 0.643\n",
      "[SpecAug] Epoch 19 | Train Acc: 0.615 | Val Acc: 0.651\n",
      "[SpecAug] Epoch 20 | Train Acc: 0.585 | Val Acc: 0.684\n",
      "[SpecAug] Epoch 21 | Train Acc: 0.608 | Val Acc: 0.648\n",
      "[SpecAug] Epoch 22 | Train Acc: 0.602 | Val Acc: 0.687\n",
      "[SpecAug] Epoch 23 | Train Acc: 0.611 | Val Acc: 0.665\n",
      "[SpecAug] Epoch 24 | Train Acc: 0.612 | Val Acc: 0.703\n",
      "[SpecAug] Epoch 25 | Train Acc: 0.594 | Val Acc: 0.673\n",
      "[SpecAug] Epoch 26 | Train Acc: 0.623 | Val Acc: 0.646\n",
      "[SpecAug] Epoch 27 | Train Acc: 0.642 | Val Acc: 0.700\n",
      "[SpecAug] Epoch 28 | Train Acc: 0.617 | Val Acc: 0.698\n",
      "[SpecAug] Epoch 29 | Train Acc: 0.617 | Val Acc: 0.676\n",
      "[SpecAug] Epoch 30 | Train Acc: 0.624 | Val Acc: 0.673\n",
      "[SpecAug] Epoch 31 | Train Acc: 0.623 | Val Acc: 0.711\n",
      "[SpecAug] Epoch 32 | Train Acc: 0.622 | Val Acc: 0.706\n",
      "[SpecAug] Epoch 33 | Train Acc: 0.619 | Val Acc: 0.687\n",
      "[SpecAug] Epoch 34 | Train Acc: 0.623 | Val Acc: 0.700\n",
      "[SpecAug] Epoch 35 | Train Acc: 0.611 | Val Acc: 0.705\n",
      "[SpecAug] Epoch 36 | Train Acc: 0.633 | Val Acc: 0.698\n",
      "[SpecAug] Epoch 37 | Train Acc: 0.631 | Val Acc: 0.716\n",
      "[SpecAug] Epoch 38 | Train Acc: 0.622 | Val Acc: 0.687\n",
      "[SpecAug] Epoch 39 | Train Acc: 0.624 | Val Acc: 0.698\n",
      "[SpecAug] Epoch 40 | Train Acc: 0.613 | Val Acc: 0.698\n",
      "[SpecAug] Epoch 41 | Train Acc: 0.627 | Val Acc: 0.706\n",
      "[SpecAug] Epoch 42 | Train Acc: 0.635 | Val Acc: 0.711\n",
      "[SpecAug] Epoch 43 | Train Acc: 0.628 | Val Acc: 0.694\n",
      "[SpecAug] Epoch 44 | Train Acc: 0.627 | Val Acc: 0.684\n",
      "[SpecAug] Epoch 45 | Train Acc: 0.632 | Val Acc: 0.698\n",
      "[SpecAug] Epoch 46 | Train Acc: 0.634 | Val Acc: 0.697\n",
      "[SpecAug] Epoch 47 | Train Acc: 0.617 | Val Acc: 0.703\n",
      "[SpecAug] Epoch 48 | Train Acc: 0.625 | Val Acc: 0.716\n",
      "[SpecAug] Epoch 49 | Train Acc: 0.627 | Val Acc: 0.700\n",
      "[SpecAug] Epoch 50 | Train Acc: 0.619 | Val Acc: 0.694\n",
      "\n",
      "✔️ Best val_acc = 0.716, saved to best_speccnn.pt\n",
      "\n",
      "Final Validation → Loss: 0.9676, Acc: 0.716\n",
      "✅ submission.csv created with 551 rows\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Train+Checkpoint → Load Best → Eval → Submission\n",
    "\n",
    "import os, csv\n",
    "import torch\n",
    "import torchaudio\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchaudio.transforms import FrequencyMasking, TimeMasking\n",
    "\n",
    "best_val_acc = 0.0\n",
    "ckpt_path    = \"best_speccnn.pt\"\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    tr_loss, tr_acc, _ = train_one_epoch_spec(\n",
    "        model_spec, spec_aug_loader, optimizer_spec, device\n",
    "    )\n",
    "    vl_loss, vl_acc    = evaluate_spec(\n",
    "        model_spec, spec_val_loader, device\n",
    "    )\n",
    "    scheduler_spec.step(vl_loss)\n",
    "\n",
    "    # checkpoint on improvement\n",
    "    if vl_acc > best_val_acc + 1e-4:\n",
    "        best_val_acc = vl_acc\n",
    "        torch.save(model_spec.state_dict(), ckpt_path)\n",
    "\n",
    "    print(f\"[SpecAug] Epoch {epoch:02d} | \"\n",
    "          f\"Train Acc: {tr_acc:.3f} | Val Acc: {vl_acc:.3f}\")\n",
    "\n",
    "print(f\"\\n✔️ Best val_acc = {best_val_acc:.3f}, saved to {ckpt_path}\")\n",
    "\n",
    "############################################\n",
    "# 2) Load best weights & final validation\n",
    "############################################\n",
    "model_spec.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model_spec.eval()\n",
    "\n",
    "final_loss, final_acc = evaluate_spec(model_spec, spec_val_loader, device)\n",
    "print(f\"\\nFinal Validation → Loss: {final_loss:.4f}, Acc: {final_acc:.3f}\")\n",
    "\n",
    "############################################\n",
    "# 3) Build test loader and write submission.csv\n",
    "############################################\n",
    "class SpecTestDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform):\n",
    "        self.paths = sorted(glob(os.path.join(data_dir, \"*.wav\")))\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        if sr != 16000:\n",
    "            wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "        wav = (wav - wav.mean())/(wav.std()+1e-9)\n",
    "        spec = self.transform(wav)\n",
    "        spec = torch.log(spec + 1e-9)\n",
    "        return spec, os.path.basename(path)\n",
    "\n",
    "def test_collate(batch):\n",
    "    specs, fnames = zip(*batch)\n",
    "    T_max = max(s.size(2) for s in specs)\n",
    "    padded = [torch.nn.functional.pad(s, (0, T_max-s.size(2))) for s in specs]\n",
    "    return torch.stack(padded, 0), list(fnames)\n",
    "\n",
    "# instantiate & run\n",
    "test_ds     = SpecTestDataset(TEST_DIR, mel_tf)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False,\n",
    "                         num_workers=0, pin_memory=False,\n",
    "                         collate_fn=test_collate)\n",
    "\n",
    "filenames, accents = [], []\n",
    "with torch.no_grad():\n",
    "    for specs, fnames in test_loader:\n",
    "        specs = specs.to(device)\n",
    "        preds = model_spec(specs).argmax(dim=1).cpu().tolist()\n",
    "        filenames.extend(fnames)\n",
    "        accents.extend([p+1 for p in preds])\n",
    "\n",
    "# write CSV\n",
    "with open(\"submission.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"filename\", \"accent\"])\n",
    "    writer.writerows(zip(filenames, accents))\n",
    "\n",
    "print(f\"✅ submission.csv created with {len(accents)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7b51767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Accents):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Accent 1       0.90      0.90      0.90       146\n",
      "    Accent 2       0.82      0.71      0.76       126\n",
      "    Accent 3       0.66      0.67      0.66       113\n",
      "    Accent 4       0.93      0.66      0.77       156\n",
      "    Accent 5       0.35      0.58      0.44        92\n",
      "\n",
      "    accuracy                           0.72       633\n",
      "   macro avg       0.73      0.70      0.71       633\n",
      "weighted avg       0.77      0.72      0.73       633\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[132   1   1   0  12]\n",
      " [  1  89   5   0  31]\n",
      " [  7   3  76   0  27]\n",
      " [  1   3  21 103  28]\n",
      " [  6  12  13   8  53]]\n"
     ]
    }
   ],
   "source": [
    "# Cell: Detailed per‐accent breakdown\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make sure evaluate_and_report is defined; if not, re‐define it as:\n",
    "def evaluate_and_report(model, loader, device):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_genders = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, accents, genders in loader:\n",
    "            specs = specs.to(device)\n",
    "            logits = model(specs)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(accents.numpy())\n",
    "            all_genders.append(genders.numpy())\n",
    "\n",
    "    all_preds  = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    print(\"Classification Report (Accents):\")\n",
    "    print(classification_report(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        labels=[0,1,2,3,4],\n",
    "        target_names=[f\"Accent {i}\" for i in range(1,6)],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds, labels=[0,1,2,3,4]))\n",
    "\n",
    "# Run it:\n",
    "evaluate_and_report(model_spec, spec_val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
