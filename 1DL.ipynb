{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416bf165",
   "metadata": {},
   "source": [
    "# Task 1 - Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f93f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ecd2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset + helpers\n",
    "\n",
    "class AccentAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lazily loads .wav files and extracts accent & gender from filename.\n",
    "    Filenames like '3f_utterance123.wav' where:\n",
    "      - first char = accent [1-5]\n",
    "      - second char = gender 'm' or 'f'\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.filepaths = sorted(glob(os.path.join(data_dir, '*.wav')))\n",
    "        if not self.filepaths:\n",
    "            raise RuntimeError(f\"[AccentAudioDataset] No .wav files in {data_dir!r}\")\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.filepaths[idx]\n",
    "        waveform, sr = torchaudio.load(path)          # [1, L]\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "        # online standardization\n",
    "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-9)\n",
    "        fname = os.path.basename(path)\n",
    "        accent = int(fname[0]) - 1                   # 0–4\n",
    "        gender = 0 if fname[1] == 'm' else 1\n",
    "        return waveform.squeeze(0), accent, gender\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\" Pad to batch-max length, return lengths + labels \"\"\"\n",
    "    waves, accents, genders = zip(*batch)\n",
    "    lengths = torch.tensor([w.size(0) for w in waves], dtype=torch.long)\n",
    "    padded  = pad_sequence(waves, batch_first=True)\n",
    "    return padded, lengths, torch.tensor(accents), torch.tensor(genders)\n",
    "\n",
    "def compute_dataset_stats(data_dir, sample_rate=16000):\n",
    "    \"\"\" Scan WAVs for duration stats; errors if folder’s empty \"\"\"\n",
    "    paths = glob(os.path.join(data_dir, '*.wav'))\n",
    "    if not paths:\n",
    "        raise RuntimeError(f\"[compute_dataset_stats] No .wav files in {data_dir!r}\")\n",
    "    durations = torch.tensor([torchaudio.info(p).num_frames / sample_rate for p in paths])\n",
    "    return {\n",
    "        'count': len(durations),\n",
    "        'min_s':  float(durations.min()),\n",
    "        'max_s':  float(durations.max()),\n",
    "        'mean_s': float(durations.mean()),\n",
    "        'p90_s':  float(durations.kthvalue(int(0.9*len(durations))).values)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec885ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DIR: /Users/bramdewaal/Desktop/Uni/VSC/Deep Learning/Assignment/Train → exists? True\n",
      "  sample files: ['2m_9039.wav', '4f_1887.wav', '4f_9571.wav', '1m_3736.wav', '1m_3078.wav']\n",
      " TEST_DIR: /Users/bramdewaal/Desktop/Uni/VSC/Deep Learning/Assignment/Test → exists? True\n",
      "  sample files: ['9430.wav', '4458.wav', '1534.wav', '8510.wav', '7192.wav']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Point to your folders & verify\n",
    "# ↳ Copy paths exactly, no trailing spaces ↓\n",
    "TRAIN_DIR = \"/Users/bramdewaal/Desktop/Uni/VSC/Deep Learning/Assignment/Train\"\n",
    "TEST_DIR  = \"/Users/bramdewaal/Desktop/Uni/VSC/Deep Learning/Assignment/Test\"\n",
    "\n",
    "# Strip accidental whitespace just in case\n",
    "TRAIN_DIR = TRAIN_DIR.strip()\n",
    "TEST_DIR  = TEST_DIR.strip()\n",
    "\n",
    "print(\"TRAIN_DIR:\", TRAIN_DIR, \"→ exists?\", os.path.isdir(TRAIN_DIR))\n",
    "print(\"  sample files:\", os.listdir(TRAIN_DIR)[:5])\n",
    "print(\" TEST_DIR:\", TEST_DIR, \"→ exists?\", os.path.isdir(TEST_DIR))\n",
    "print(\"  sample files:\", os.listdir(TEST_DIR)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfb1f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch → waves: torch.Size([32, 141994]), lengths: torch.Size([32])\n",
      "Test batch → waves: torch.Size([32, 150186]), lengths: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 (fast sanity check): instantiate datasets, build loaders & grab one batch\n",
    "\n",
    "# 1) Instantiate datasets\n",
    "train_ds = AccentAudioDataset(TRAIN_DIR)\n",
    "test_ds  = AccentAudioDataset(TEST_DIR)\n",
    "\n",
    "# 2) Create DataLoaders with num_workers=0 to avoid spawn overhead in Jupyter\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,       # no worker processes\n",
    "    pin_memory=False,    # lower overhead in notebook\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 3) Sanity-check one batch from each loader\n",
    "for name, loader in [(\"Train\", train_loader), (\"Test\", test_loader)]:\n",
    "    waves, lengths, accents, genders = next(iter(loader))\n",
    "    print(f\"{name} batch → waves: {waves.shape}, lengths: {lengths.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132bc6d",
   "metadata": {},
   "source": [
    "# Task 2 - Raw signal 1d cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed42808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define Raw-Signal 1D CNN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RawCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    1D CNN for raw waveform classification (5 accents).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=5, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Conv stage 1: 1→16 channels, downsample by 4\n",
    "            nn.Conv1d(1, 16, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2),  # seq_len / 4 total\n",
    "\n",
    "            # Conv stage 2: 16→32, downsample by 2\n",
    "            nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2),  # seq_len / 8 total\n",
    "\n",
    "            # Conv stage 3: 32→64, downsample by 2\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2),  # seq_len / 16 total\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(1),  # collapse time dim\n",
    "            nn.Flatten(),             # → [B, 64]\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: FloatTensor [B, T] raw wave\n",
    "        \"\"\"\n",
    "        # add channel dim\n",
    "        x = x.unsqueeze(1)  # [B,1,T]\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c140765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "✅ RawCNN forward pass on mps OK: torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Device selection & forward pass test (using MPS if available)\n",
    "\n",
    "import torch\n",
    "\n",
    "# 1) Select device: MPS (Apple GPU) if available, else CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 2) Move model to device\n",
    "model = RawCNN(num_classes=5, dropout=0.2).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 3) Grab one batch, move to device, and test forward pass\n",
    "with torch.no_grad():\n",
    "    waves, lengths, accents, genders = next(iter(train_loader))\n",
    "    waves = waves.to(device)\n",
    "    logits = model(waves)\n",
    "    assert logits.shape == (waves.size(0), 5), f\"Expected [B,5], got {logits.shape}\"\n",
    "    print(f\"✅ RawCNN forward pass on {device} OK: {logits.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfb5c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train   → loss: 1.5678, acc: 0.297, time: 17.8s\n",
      "Validate→ loss: 0.8872, acc: 0.105\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Training & Evaluation Functions with MPS Support\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "    for waves, lengths, accents, _ in loader:\n",
    "        waves, accents = waves.to(device), accents.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(waves)\n",
    "        loss = F.cross_entropy(logits, accents)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * waves.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == accents).sum().item()\n",
    "        total += waves.size(0)\n",
    "    elapsed = time.time() - start\n",
    "    return total_loss / total, correct / total, elapsed\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for waves, lengths, accents, _ in loader:\n",
    "        waves, accents = waves.to(device), accents.to(device)\n",
    "        logits = model(waves)\n",
    "        loss = F.cross_entropy(logits, accents)\n",
    "\n",
    "        total_loss += loss.item() * waves.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == accents).sum().item()\n",
    "        total += waves.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# Quick function test (one batch)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model = RawCNN(num_classes=5, dropout=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loss, train_acc, train_time = train_one_epoch(model, train_loader, optimizer, device)\n",
    "val_loss,   val_acc   = evaluate(model, test_loader, device)\n",
    "print(f\"Train   → loss: {train_loss:.4f}, acc: {train_acc:.3f}, time: {train_time:.1f}s\")\n",
    "print(f\"Validate→ loss: {val_loss:.4f}, acc: {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71af6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 1.5071, Acc: 0.383 | Val   Loss: 0.8946, Acc: 0.109 | Time: 21.0s\n",
      "Epoch 02 | Train Loss: 1.4541, Acc: 0.426 | Val   Loss: 0.9242, Acc: 0.102 | Time: 23.1s\n",
      "Epoch 03 | Train Loss: 1.3959, Acc: 0.449 | Val   Loss: 0.9141, Acc: 0.111 | Time: 27.2s\n",
      "Epoch 04 | Train Loss: 1.3547, Acc: 0.459 | Val   Loss: 0.9344, Acc: 0.116 | Time: 25.0s\n",
      "⏹ Early stopping at epoch 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Full Training Loop with Scheduler & Early Stopping\n",
    "\n",
    "num_epochs = 5\n",
    "patience = 2\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Scheduler: halve LR when val loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       mode='min',\n",
    "                                                       factor=0.5,\n",
    "                                                       patience=1)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_start = time.time()\n",
    "    train_loss, train_acc, train_time = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, device)\n",
    "    scheduler.step(val_loss)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.3f} | \"\n",
    "          f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.3f} | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve > patience:\n",
    "            print(f\"⏹ Early stopping at epoch {epoch}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b440833",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68f0dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrogram batch → torch.Size([32, 1, 64, 657]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Spectrogram Dataset & DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Mel-spectrogram transform\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=16000, n_fft=512, hop_length=256, n_mels=64\n",
    ")\n",
    "\n",
    "class SpectrogramDataset(AccentAudioDataset):\n",
    "    def __init__(self, data_dir, mel_transform):\n",
    "        super().__init__(data_dir)\n",
    "        self.mel_transform = mel_transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, accent, gender = super().__getitem__(idx)\n",
    "        spec = self.mel_transform(waveform.unsqueeze(0))  # [1, n_mels, frames]\n",
    "        spec = torch.log(spec + 1e-9)\n",
    "        return spec, accent, gender\n",
    "\n",
    "def spectrogram_collate_fn(batch):\n",
    "    specs, accents, genders = zip(*batch)\n",
    "    lengths = torch.tensor([s.size(2) for s in specs], dtype=torch.long)\n",
    "    T_max = lengths.max().item()\n",
    "    padded = [F.pad(s, (0, T_max - s.size(2))) for s in specs]\n",
    "    padded = torch.stack(padded)  # [B,1,n_mels,T_max]\n",
    "    return padded, lengths, torch.tensor(accents), torch.tensor(genders)\n",
    "\n",
    "# Instantiate loaders\n",
    "spec_train_ds = SpectrogramDataset(TRAIN_DIR, mel_transform)\n",
    "spec_test_ds  = SpectrogramDataset(TEST_DIR,  mel_transform)\n",
    "\n",
    "spec_train_loader = DataLoader(\n",
    "    spec_train_ds, batch_size=32, shuffle=True,\n",
    "    num_workers=0, pin_memory=False,\n",
    "    collate_fn=spectrogram_collate_fn\n",
    ")\n",
    "spec_test_loader = DataLoader(\n",
    "    spec_test_ds, batch_size=32, shuffle=False,\n",
    "    num_workers=0, pin_memory=False,\n",
    "    collate_fn=spectrogram_collate_fn\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "specs, lengths, accents, genders = next(iter(spec_train_loader))\n",
    "print(\"Spectrogram batch →\", specs.shape, lengths.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27169fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SpectrogramCNN forward: torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Spectrogram CNN Definition & Forward Test\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpectrogramCNN(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Forward‐pass test on MPS/CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model = SpectrogramCNN().to(device)\n",
    "with torch.no_grad():\n",
    "    specs = specs.to(device)\n",
    "    logits = model(specs)\n",
    "    print(\"✅ SpectrogramCNN forward:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a73262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "SpectrogramCNN → Train loss: 1.5469, acc: 0.315\n",
      "                 Val   loss: 0.9278, acc: 0.109\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Train & Eval SpectrogramCNN on MPS/CPU (one‐batch smoke test)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1) Device\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 2) Instantiate model, optimizer\n",
    "model_spec = SpectrogramCNN(num_classes=5, dropout=0.3).to(device)\n",
    "optimizer_spec = optim.Adam(model_spec.parameters(), lr=1e-3)\n",
    "\n",
    "# 3) Quick one‐batch train & eval\n",
    "train_loss, train_acc, _ = train_one_epoch(model_spec, spec_train_loader, optimizer_spec, device)\n",
    "val_loss,   val_acc       = evaluate   (model_spec, spec_test_loader,        device)\n",
    "\n",
    "print(f\"SpectrogramCNN → Train loss: {train_loss:.4f}, acc: {train_acc:.3f}\")\n",
    "print(f\"                 Val   loss: {val_loss:.4f}, acc: {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03baeb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 1.4304, Acc: 0.437 | Val   Loss: 0.9086, Acc: 0.120 | Time: 18.7s\n",
      "Epoch 02 | Train Loss: 1.3113, Acc: 0.505 | Val   Loss: 0.9614, Acc: 0.118 | Time: 18.1s\n",
      "Epoch 03 | Train Loss: 1.2123, Acc: 0.543 | Val   Loss: 0.9655, Acc: 0.114 | Time: 13.3s\n",
      "Epoch 04 | Train Loss: 1.1359, Acc: 0.582 | Val   Loss: 0.9930, Acc: 0.114 | Time: 12.9s\n",
      "⏹ Early stopping at epoch 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Full Training Loop for SpectrogramCNN with Scheduler & Early Stopping\n",
    "\n",
    "num_epochs = 5\n",
    "patience   = 2\n",
    "best_val   = float('inf')\n",
    "no_improve = 0\n",
    "\n",
    "# Scheduler to reduce LR on plateau\n",
    "scheduler_spec = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_spec, mode='min', factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc, tr_time = train_one_epoch(\n",
    "        model_spec, spec_train_loader, optimizer_spec, device\n",
    "    )\n",
    "    vl_loss, vl_acc = evaluate(model_spec, spec_test_loader, device)\n",
    "    scheduler_spec.step(vl_loss)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train Loss: {tr_loss:.4f}, Acc: {tr_acc:.3f} | \"\n",
    "          f\"Val   Loss: {vl_loss:.4f}, Acc: {vl_acc:.3f} | \"\n",
    "          f\"Time: {dt:.1f}s\")\n",
    "\n",
    "    # early stopping\n",
    "    if vl_loss < best_val - 1e-4:\n",
    "        best_val   = vl_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve > patience:\n",
    "            print(f\"⏹ Early stopping at epoch {epoch}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0255d",
   "metadata": {},
   "source": [
    "# Task 3 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6f40232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do=0.1, wd=0.0 → val_acc=0.132\n",
      "do=0.1, wd=0.0001 → val_acc=0.113\n",
      "do=0.1, wd=0.001 → val_acc=0.123\n",
      "do=0.3, wd=0.0 → val_acc=0.116\n",
      "do=0.3, wd=0.0001 → val_acc=0.125\n",
      "do=0.3, wd=0.001 → val_acc=0.125\n",
      "do=0.5, wd=0.0 → val_acc=0.123\n",
      "do=0.5, wd=0.0001 → val_acc=0.129\n",
      "do=0.5, wd=0.001 → val_acc=0.118\n",
      "   dropout  weight_decay   val_acc  train_acc\n",
      "0      0.1        0.0000  0.132486   0.565066\n",
      "1      0.1        0.0001  0.112523   0.610865\n",
      "2      0.1        0.0010  0.123412   0.590335\n",
      "3      0.3        0.0000  0.116152   0.529375\n",
      "4      0.3        0.0001  0.125227   0.553380\n",
      "5      0.3        0.0010  0.125227   0.520846\n",
      "6      0.5        0.0000  0.123412   0.467151\n",
      "7      0.5        0.0001  0.128857   0.501579\n",
      "8      0.5        0.0010  0.117967   0.510107\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Regularization Experiments on SpectrogramCNN\n",
    "\n",
    "import itertools\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameter grid\n",
    "dropouts     = [0.1, 0.3, 0.5]\n",
    "weight_decays = [0.0, 1e-4, 1e-3]\n",
    "\n",
    "results = []\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "for do, wd in itertools.product(dropouts, weight_decays):\n",
    "    # 1) Instantiate model & optimizer with wd\n",
    "    model = SpectrogramCNN(num_classes=5, dropout=do).to(device)\n",
    "    optim_spec = optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optim_spec, mode='min', factor=0.5, patience=1\n",
    "    )\n",
    "\n",
    "    # 2) Train with early stopping for up to 5 epochs\n",
    "    best_val = float('inf')\n",
    "    no_improve = 0\n",
    "    for epoch in range(1, 6):\n",
    "        tr_loss, tr_acc, _ = train_one_epoch(model, spec_train_loader, optim_spec, device)\n",
    "        vl_loss, vl_acc    = evaluate(model, spec_test_loader,       device)\n",
    "        scheduler.step(vl_loss)\n",
    "        if vl_loss < best_val - 1e-4:\n",
    "            best_val, no_improve = vl_loss, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve > 2:\n",
    "            break\n",
    "\n",
    "    # 3) Record final val accuracy & time/epoch ~ from spec loop timings\n",
    "    results.append({\n",
    "        'dropout': do,\n",
    "        'weight_decay': wd,\n",
    "        'val_acc': vl_acc,\n",
    "        'train_acc': tr_acc\n",
    "    })\n",
    "    print(f\"do={do}, wd={wd} → val_acc={vl_acc:.3f}\")\n",
    "\n",
    "# 4) Tabulate results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6431be4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RawCNN] Epoch 01 | Train Loss: 1.5761, Acc: 0.280 | Val   Loss: 0.8762, Acc: 0.116 | Time: 26.6s\n",
      "[RawCNN] Epoch 02 | Train Loss: 1.5150, Acc: 0.370 | Val   Loss: 0.9294, Acc: 0.107 | Time: 24.3s\n",
      "[RawCNN] Epoch 03 | Train Loss: 1.4695, Acc: 0.409 | Val   Loss: 0.9197, Acc: 0.109 | Time: 35.9s\n",
      "[RawCNN] Epoch 04 | Train Loss: 1.4222, Acc: 0.434 | Val   Loss: 0.8987, Acc: 0.109 | Time: 35.1s\n",
      "⏹ Early stopping at epoch 4\n",
      "✔️ Finished RawCNN training: best val_acc = 0.109\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Train RawCNN with Best Regularization & Early Stopping\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparams from SpectrogramCNN grid: dropout=0.3, weight_decay=1e-3\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model_raw = RawCNN(num_classes=5, dropout=0.3).to(device)\n",
    "optimizer_raw = optim.Adam(model_raw.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "scheduler_raw = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_raw, mode='min', factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "# Training loop (up to 5 epochs, patience=2)\n",
    "num_epochs = 5\n",
    "patience   = 2\n",
    "best_val   = float('inf')\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc, tr_time = train_one_epoch(\n",
    "        model_raw, train_loader, optimizer_raw, device\n",
    "    )\n",
    "    vl_loss, vl_acc = evaluate(model_raw, test_loader, device)\n",
    "    scheduler_raw.step(vl_loss)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    print(f\"[RawCNN] Epoch {epoch:02d} | \"\n",
    "          f\"Train Loss: {tr_loss:.4f}, Acc: {tr_acc:.3f} | \"\n",
    "          f\"Val   Loss: {vl_loss:.4f}, Acc: {vl_acc:.3f} | \"\n",
    "          f\"Time: {dt:.1f}s\")\n",
    "\n",
    "    # Early stopping\n",
    "    if vl_loss < best_val - 1e-4:\n",
    "        best_val, no_improve = vl_loss, 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve > patience:\n",
    "            print(f\"⏹ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(f\"✔️ Finished RawCNN training: best val_acc = {vl_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccd785dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Accents):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Accent 1       0.10      0.54      0.18        54\n",
      "    Accent 2       0.10      0.03      0.05        58\n",
      "    Accent 3       0.00      0.00      0.00        65\n",
      "    Accent 4       0.12      0.44      0.19        62\n",
      "    Accent 5       0.07      0.03      0.05        59\n",
      "\n",
      "   micro avg       0.11      0.20      0.14       298\n",
      "   macro avg       0.08      0.21      0.09       298\n",
      "weighted avg       0.08      0.20      0.09       298\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[29  0  0 23  2]\n",
      " [28  2  0 24  4]\n",
      " [30  2  0 31  2]\n",
      " [29  3  0 27  3]\n",
      " [26  3  0 28  2]]\n",
      "\n",
      "Gender-specific accuracy:\n",
      "  Male: N/A (0 samples)\n",
      "  Female: 0.109 (551 samples)\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Detailed Performance Evaluation by Accent & Gender (fully self-contained)\n",
    "\n",
    "def evaluate_and_report(model, loader, device):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_genders = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for waves, lengths, accents, genders in loader:\n",
    "            waves = waves.to(device)\n",
    "            logits = model(waves)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(accents.numpy())\n",
    "            all_genders.append(genders.numpy())\n",
    "\n",
    "    all_preds   = np.concatenate(all_preds)\n",
    "    all_labels  = np.concatenate(all_labels)\n",
    "    all_genders = np.concatenate(all_genders)\n",
    "\n",
    "    labels       = [0, 1, 2, 3, 4]\n",
    "    target_names = [f\"Accent {i}\" for i in range(1, 6)]\n",
    "\n",
    "    print(\"Classification Report (Accents):\")\n",
    "    print(classification_report(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        labels=labels,\n",
    "        target_names=target_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds, labels=labels))\n",
    "\n",
    "    print(\"\\nGender-specific accuracy:\")\n",
    "    for val, name in zip([0, 1], [\"Male\", \"Female\"]):\n",
    "        idxs = all_genders == val\n",
    "        count = idxs.sum()\n",
    "        if count > 0:\n",
    "            acc = (all_preds[idxs] == all_labels[idxs]).mean()\n",
    "            print(f\"  {name}: {acc:.3f} ({count} samples)\")\n",
    "        else:\n",
    "            print(f\"  {name}: N/A ({count} samples)\")\n",
    "\n",
    "# Run it:\n",
    "evaluate_and_report(model_raw, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ea7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
