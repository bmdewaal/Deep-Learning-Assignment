{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1875b3c1",
   "metadata": {},
   "source": [
    "# Deep Learning assignment\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac379ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Use MPS on Apple Silicon if available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66954fa6",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf823d6",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11a1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, data_dir, sample_rate=16000, max_length=16000, num_classes=5):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length  = max_length\n",
    "        self.num_classes = num_classes\n",
    "        self.files = []\n",
    "\n",
    "        for root, _, fns in os.walk(data_dir):\n",
    "            for fn in sorted(fns):\n",
    "                if not fn.lower().endswith('.wav'):\n",
    "                    continue\n",
    "                label = int(fn[0]) - 1\n",
    "                # keep only accents 1–5 → labels 0–4\n",
    "                if 0 <= label < self.num_classes:\n",
    "                    self.files.append(os.path.join(root, fn))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # mono\n",
    "        if waveform.size(1) < self.max_length:\n",
    "            pad = self.max_length - waveform.size(1)\n",
    "            waveform = nn.functional.pad(waveform, (0, pad))\n",
    "        else:\n",
    "            waveform = waveform[:, :self.max_length]\n",
    "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-5)\n",
    "        label = int(os.path.basename(path)[0]) - 1\n",
    "        return waveform.to(device), torch.tensor(label, dtype=torch.long, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb683cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Train'        \n",
    "test_dir  = 'Test set'     \n",
    "\n",
    "train_ds     = AccentDataset(train_dir,  num_classes=5)\n",
    "test_ds      = AccentDataset(test_dir,   num_classes=5)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556ad2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train files: 3166\n",
      "Duration (s): min=1.67, max=12.97, mean=5.26, median=5.04\n",
      "Accent distribution: Counter({4: 754, 1: 740, 2: 626, 3: 564, 5: 482})\n",
      "Gender distribution: Counter({'f': 1639, 'm': 1527})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "durations = []\n",
    "accents   = []\n",
    "genders   = []\n",
    "\n",
    "for path in train_ds.files:\n",
    "    info = torchaudio.info(path)\n",
    "    dur = info.num_frames / info.sample_rate\n",
    "    durations.append(dur)\n",
    "    fn = os.path.basename(path)\n",
    "    accents.append(int(fn[0]))\n",
    "    genders.append(fn[1])\n",
    "\n",
    "# 2) Print stats\n",
    "print(f\"Total train files: {len(durations)}\")\n",
    "print(f\"Duration (s): min={min(durations):.2f}, max={max(durations):.2f}, \"\n",
    "      f\"mean={np.mean(durations):.2f}, median={np.median(durations):.2f}\")\n",
    "\n",
    "print(\"Accent distribution:\", Counter(accents))\n",
    "print(\"Gender distribution:\", Counter(genders))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832f9b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axes(0.125,0.11;0.775x0.77)\n",
      "gender    f    m\n",
      "accent          \n",
      "1       520  220\n",
      "2       287  339\n",
      "3       242  322\n",
      "4       357  397\n",
      "5       233  249\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"duration\": durations,\n",
    "    \"accent\":   accents,\n",
    "    \"gender\":   genders,\n",
    "})\n",
    "\n",
    "\n",
    "print(df.duration.hist(bins=10))\n",
    "\n",
    "\n",
    "print(df.groupby([\"accent\",\"gender\"]).size().unstack(fill_value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf8eaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3166 train files, 298 test files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(f\"Found {len(train_ds)} train files, {len(test_ds)} test files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6e9f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durations (s): min=1.67, median=5.03, max=12.97\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: gather durations (in seconds) and basic stats\n",
    "durations = []\n",
    "for path in train_ds.files:\n",
    "    info = torchaudio.info(path)\n",
    "    durations.append(info.num_frames / info.sample_rate)\n",
    "\n",
    "durations = torch.tensor(durations)\n",
    "print(f\"Durations (s): min={durations.min():.2f}, \"\n",
    "      f\"median={durations.median():.2f}, max={durations.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ace593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: histogram of utterance lengths\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(durations.numpy(), bins=20, edgecolor='black')\n",
    "plt.xlabel(\"Duration (s)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Train-set Utterance Duration Distribution\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e34b5bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accent</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accent gender\n",
       "0       1      f\n",
       "1       1      f\n",
       "2       1      f\n",
       "3       1      f\n",
       "4       1      f"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for path in train_ds.files:\n",
    "    fn = os.path.basename(path)\n",
    "    accent = int(fn[0])\n",
    "    gender = fn[1]\n",
    "    rows.append({\"accent\": accent, \"gender\": gender})\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3fee08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts per (accent, gender):\n",
      "gender    f    m\n",
      "accent          \n",
      "1       520  220\n",
      "2       287  339\n",
      "3       242  322\n",
      "4       357  397\n",
      "5       233  249\n"
     ]
    }
   ],
   "source": [
    "ct = pd.crosstab(df.accent, df.gender)\n",
    "print(\"Counts per (accent, gender):\")\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b69ee2",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f4ec3",
   "metadata": {},
   "source": [
    "## Dataset import/definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25f02b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, data_dir, sample_rate=16000, max_length=16000, num_classes=5):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length  = max_length\n",
    "        self.num_classes = num_classes\n",
    "        self.files = []\n",
    "\n",
    "        for root, _, fns in os.walk(data_dir):\n",
    "            for fn in sorted(fns):\n",
    "                if not fn.lower().endswith('.wav'):\n",
    "                    continue\n",
    "                label = int(fn[0]) - 1\n",
    "                # keep only accents 1–5 → labels 0–4\n",
    "                if 0 <= label < self.num_classes:\n",
    "                    self.files.append(os.path.join(root, fn))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # mono\n",
    "        if waveform.size(1) < self.max_length:\n",
    "            pad = self.max_length - waveform.size(1)\n",
    "            waveform = nn.functional.pad(waveform, (0, pad))\n",
    "        else:\n",
    "            waveform = waveform[:, :self.max_length]\n",
    "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-5)\n",
    "        label = int(os.path.basename(path)[0]) - 1\n",
    "        return waveform.to(device), torch.tensor(label, dtype=torch.long, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fcd637",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b8398",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "The dataloaders are used for..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55d49fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Train'        \n",
    "test_dir  = 'Test set'     \n",
    "\n",
    "train_ds     = AccentDataset(train_dir,  num_classes=5)\n",
    "test_ds      = AccentDataset(test_dir,   num_classes=5)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6fee6a",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ba5a9",
   "metadata": {},
   "source": [
    "## Model - CNN (task 1.2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52c126d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RawCNN(nn.Module):\n",
    "#     def __init__(self, num_classes=5, max_length=16000):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv1d(1, 16, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(2),\n",
    "#             nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(2),\n",
    "#         )\n",
    "#         reduced = max_length // 4  # two poolings of 2\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(32 * reduced, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(128, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc(self.conv(x))\n",
    "\n",
    "# model = RawCNN(num_classes=5, max_length=train_ds.max_length).to(device)\n",
    "\n",
    "class RawCNNVarLenDeep(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # block 1\n",
    "            nn.Conv1d(1,   64, kernel_size=7, padding=3),  # wider kernel, more feature-maps\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            # block 2\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            # block 3\n",
    "            nn.Conv1d(128,256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "        # still reduce variable-length to 10 timesteps\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(10)\n",
    "\n",
    "        # adjust classifier input dim = 256 channels × 10 timesteps\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),               # [batch, 256*10 = 2560]\n",
    "            nn.Linear(256*10, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = RawCNNVarLenDeep(num_classes=5).to(device)\n",
    "\n",
    "# Loss and optimizer:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8222ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = correct = total = 0\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for X, y in loader:\n",
    "            logits = model(X)\n",
    "            loss   = criterion(logits, y)\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total   += X.size(0)\n",
    "    return total_loss/total, correct/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b92f4",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "308d2c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 1.6158, Acc: 0.2495 | Test  Loss: 1.7782, Acc: 0.1812\n",
      "Epoch 02 | Train Loss: 1.5396, Acc: 0.2884 | Test  Loss: 1.7434, Acc: 0.2047\n",
      "Epoch 03 | Train Loss: 1.5277, Acc: 0.2997 | Test  Loss: 1.7341, Acc: 0.1946\n",
      "Epoch 04 | Train Loss: 1.5085, Acc: 0.3133 | Test  Loss: 1.7630, Acc: 0.1913\n",
      "Epoch 05 | Train Loss: 1.5079, Acc: 0.3067 | Test  Loss: 1.8297, Acc: 0.1711\n",
      "Epoch 06 | Train Loss: 1.4825, Acc: 0.3307 | Test  Loss: 1.8164, Acc: 0.1946\n",
      "Epoch 07 | Train Loss: 1.4752, Acc: 0.3320 | Test  Loss: 1.9189, Acc: 0.1779\n",
      "Epoch 08 | Train Loss: 1.4780, Acc: 0.3323 | Test  Loss: 1.7614, Acc: 0.2047\n",
      "Epoch 09 | Train Loss: 1.4498, Acc: 0.3380 | Test  Loss: 1.8337, Acc: 0.1779\n",
      "Epoch 10 | Train Loss: 1.4432, Acc: 0.3443 | Test  Loss: 1.9310, Acc: 0.2047\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    tl, ta = run_epoch(train_loader, train=True)\n",
    "    sl, sa = run_epoch(test_loader,  train=False)  \n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train Loss: {tl:.4f}, Acc: {ta:.4f} | \"\n",
    "          f\"Test  Loss: {sl:.4f}, Acc: {sa:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218ef6c",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71ac2709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: tensor([740, 626, 564, 754, 482])\n",
      "Test  labels: tensor([54, 58, 65, 62, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Train labels:\", torch.bincount(torch.tensor([y for _,y in train_ds])))\n",
    "print(\"Test  labels:\", torch.bincount(torch.tensor([y for _,y in test_ds])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5378aec",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da482f",
   "metadata": {},
   "source": [
    "## Task 1.2b - mel spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d2f5a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Cell A: Imports & device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "import os\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ca254fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell B: Log‐Mel transforms & DataLoaders\n",
    "mel_spec      = MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80)\n",
    "db_transform  = AmplitudeToDB()\n",
    "\n",
    "class LogMelDataset(Dataset):\n",
    "    def __init__(self, data_dir, mel_spec, dbt, max_length=16000, num_classes=5):\n",
    "        self.mel, self.dbt = mel_spec, dbt\n",
    "        self.max_length   = max_length\n",
    "        self.files        = []\n",
    "        for root,_,fns in os.walk(data_dir):\n",
    "            for fn in sorted(fns):\n",
    "                if fn.lower().endswith('.wav'):\n",
    "                    label = int(fn[0]) - 1\n",
    "                    if 0 <= label < num_classes:\n",
    "                        self.files.append((os.path.join(root,fn), label))\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        path,label = self.files[idx]\n",
    "        wav, sr    = torchaudio.load(path)\n",
    "        if sr!=16000:\n",
    "            wav = torchaudio.transforms.Resample(sr,16000)(wav)\n",
    "        wav = wav.mean(0,keepdim=True)\n",
    "        if wav.size(1) < self.max_length:\n",
    "            pad = self.max_length - wav.size(1)\n",
    "            wav = nn.functional.pad(wav,(0,pad))\n",
    "        else:\n",
    "            wav = wav[:,:self.max_length]\n",
    "        spec   = self.mel(wav)\n",
    "        logspec= self.dbt(spec)\n",
    "        logspec= (logspec - logspec.mean())/(logspec.std()+1e-5)\n",
    "        return logspec.to(device), torch.tensor(label, device=device)\n",
    "\n",
    "logmel_train_ds = LogMelDataset(\"Train\", mel_spec, db_transform)\n",
    "logmel_test_ds  = LogMelDataset(\"Test set\", mel_spec, db_transform)\n",
    "logmel_train_loader = DataLoader(logmel_train_ds, batch_size=32, shuffle=True)\n",
    "logmel_test_loader  = DataLoader(logmel_test_ds,  batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "# ─── Cell G: AugmentedLogMelDataset ─────────────────────────────────────\n",
    "import random\n",
    "class AugmentedLogMelDataset(LogMelDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) Load & pad/truncate waveform (same as LogMelDataset)\n",
    "        wav, label = super(LogMelDataset, self).__getitem__(idx)  # gets [1,max_len]\n",
    "        \n",
    "        # 2) Time-shift ±10%\n",
    "        max_shift = int(0.1 * self.max_length)\n",
    "        off = random.randint(-max_shift, max_shift)\n",
    "        if off>0:\n",
    "            wav = torch.cat([wav[:,off:], torch.zeros(1,off,device=wav.device)], dim=1)\n",
    "        elif off<0:\n",
    "            wav = torch.cat([torch.zeros(1,-off,device=wav.device), wav[:,:off]],   dim=1)\n",
    "        \n",
    "        # 3) Add Gaussian noise (std=0.05)\n",
    "        wav = wav + torch.randn_like(wav) * 0.05\n",
    "        \n",
    "        # 4) Recompute log-mel\n",
    "        spec    = mel_spec(wav)\n",
    "        logspec = db_transform(spec)\n",
    "        logspec = (logspec - logspec.mean())/(logspec.std()+1e-5)\n",
    "        return logspec.to(device), label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c43fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell H: rebuild augmented loader & retrain ──────────────────────────\n",
    "aug_logmel_train_ds     = AugmentedLogMelDataset(\"Train\", mel_spec, db_transform)\n",
    "aug_logmel_train_loader = DataLoader(aug_logmel_train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "# then rerun your existing train loop (Cells D–F) but with\n",
    "#   run_logmel_epoch(aug_logmel_train_loader, True)\n",
    "# instead of the clean loader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad4464af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell C: Minimal Log-Mel CNN\n",
    "class LogMel_CNN_Min(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64,128, 3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        flat_size  = 128 * (80//8) * (100//8)  # 128*10*12=15360\n",
    "        self.fc1   = nn.Linear(flat_size, 256)\n",
    "        self.fc2   = nn.Linear(256, 5)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.flatten(1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "logmel_model = LogMel_CNN_Min().to(device)\n",
    "\n",
    "\n",
    "\n",
    "# ─── Cell I: Deeper LogMel CNN ───────────────────────────────────────────\n",
    "class LogMel_CNN_Deep(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        def block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch), nn.ReLU(),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch), nn.ReLU(),\n",
    "                nn.MaxPool2d(2,2)\n",
    "            )\n",
    "        self.b1 = block(1,  32)  # → [B,32,40,50]\n",
    "        self.b2 = block(32, 64)  # → [B,64,20,25]\n",
    "        self.b3 = block(64,128)  # → [B,128,10,12]\n",
    "        flat = 128 * 10 * 12    # 15360\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat, 512), nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.b1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.b3(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "logmel_model = LogMel_CNN_Deep().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3427f0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogMel] Epoch 01 | LR 1.0e-03 | Train Acc: 0.246 | Test Acc: 0.191\n",
      "[LogMel] Epoch 02 | LR 1.0e-03 | Train Acc: 0.269 | Test Acc: 0.191\n",
      "[LogMel] Epoch 03 | LR 1.0e-03 | Train Acc: 0.321 | Test Acc: 0.198\n",
      "[LogMel] Epoch 04 | LR 1.0e-03 | Train Acc: 0.476 | Test Acc: 0.205\n",
      "[LogMel] Epoch 05 | LR 5.0e-04 | Train Acc: 0.595 | Test Acc: 0.255\n",
      "[LogMel] Epoch 06 | LR 5.0e-04 | Train Acc: 0.686 | Test Acc: 0.238\n",
      "[LogMel] Epoch 07 | LR 5.0e-04 | Train Acc: 0.723 | Test Acc: 0.228\n",
      "[LogMel] Epoch 08 | LR 5.0e-04 | Train Acc: 0.746 | Test Acc: 0.255\n",
      "[LogMel] Epoch 09 | LR 5.0e-04 | Train Acc: 0.749 | Test Acc: 0.235\n",
      "[LogMel] Epoch 10 | LR 2.5e-04 | Train Acc: 0.775 | Test Acc: 0.255\n"
     ]
    }
   ],
   "source": [
    "# Cell D: Loss, Optimizer & Scheduler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion        = nn.CrossEntropyLoss()\n",
    "logmel_optimizer = optim.Adam(\n",
    "    logmel_model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "scheduler_lm     = optim.lr_scheduler.StepLR(\n",
    "    logmel_optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "# Cell E: run_logmel_epoch Function\n",
    "def run_logmel_epoch(loader, train=True):\n",
    "    model = logmel_model\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss = correct = total = 0\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for X, y in loader:\n",
    "            logits = model(X)\n",
    "            loss   = criterion(logits, y)\n",
    "            if train:\n",
    "                logmel_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                logmel_optimizer.step()\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            preds      = logits.argmax(dim=1)\n",
    "            correct   += (preds == y).sum().item()\n",
    "            total     += X.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "# Cell F: Training Loop (Log-Mel CNN)\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    tr_loss, tr_acc = run_logmel_epoch(logmel_train_loader, train=True)\n",
    "    scheduler_lm.step()  # step the learning rate\n",
    "    te_loss, te_acc = run_logmel_epoch(logmel_test_loader, train=False)\n",
    "    print(f\"[LogMel] Epoch {epoch:02d} | \"\n",
    "          f\"LR {scheduler_lm.get_last_lr()[0]:.1e} | \"\n",
    "          f\"Train Acc: {tr_acc:.3f} | Test Acc: {te_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7dab8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell D (part 1): Loss & Optimizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "logmel_optimizer = optim.Adam(logmel_model.parameters(), lr=1e-3, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38337a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell D (part 2): Scheduler\n",
    "scheduler_lm = torch.optim.lr_scheduler.StepLR(logmel_optimizer, step_size=5, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72843925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogMel] Epoch 01 | LR 1.0e-03 | Train Acc: 0.915 | Test Acc: 0.238\n",
      "[LogMel] Epoch 02 | LR 1.0e-03 | Train Acc: 0.946 | Test Acc: 0.245\n",
      "[LogMel] Epoch 03 | LR 1.0e-03 | Train Acc: 0.957 | Test Acc: 0.248\n",
      "[LogMel] Epoch 04 | LR 1.0e-03 | Train Acc: 0.965 | Test Acc: 0.232\n",
      "[LogMel] Epoch 05 | LR 5.0e-04 | Train Acc: 0.952 | Test Acc: 0.225\n",
      "[LogMel] Epoch 06 | LR 5.0e-04 | Train Acc: 0.984 | Test Acc: 0.232\n",
      "[LogMel] Epoch 07 | LR 5.0e-04 | Train Acc: 0.993 | Test Acc: 0.228\n",
      "[LogMel] Epoch 08 | LR 5.0e-04 | Train Acc: 0.996 | Test Acc: 0.228\n",
      "[LogMel] Epoch 09 | LR 5.0e-04 | Train Acc: 0.998 | Test Acc: 0.218\n",
      "[LogMel] Epoch 10 | LR 2.5e-04 | Train Acc: 1.000 | Test Acc: 0.235\n"
     ]
    }
   ],
   "source": [
    "# Cell E & F: run_logmel_epoch & Training Loop\n",
    "def run_logmel_epoch(loader, train=True):\n",
    "    model = logmel_model\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = correct = total = 0\n",
    "    for X, y in loader:\n",
    "        logits = model(X)\n",
    "        loss   = criterion(logits, y)\n",
    "        if train:\n",
    "            logmel_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            logmel_optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        correct    += (logits.argmax(1) == y).sum().item()\n",
    "        total      += X.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    tr_loss, tr_acc = run_logmel_epoch(logmel_train_loader, True)\n",
    "    scheduler_lm.step()\n",
    "    te_loss, te_acc = run_logmel_epoch(logmel_test_loader, False)\n",
    "    print(f\"[LogMel] Epoch {epoch:02d} | \"\n",
    "          f\"LR {scheduler_lm.get_last_lr()[0]:.1e} | \"\n",
    "          f\"Train Acc: {tr_acc:.3f} | Test Acc: {te_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146d7fe",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5ac0a",
   "metadata": {},
   "source": [
    "## Task 1.2b - bad version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03310379",
   "metadata": {},
   "source": [
    "### Importing and assigning transformation from torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "573f6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "\n",
    "mfcc_transform = T.MFCC(\n",
    "    sample_rate=16000,\n",
    "    n_mfcc=40,            # common choice: 13–40 coefficients\n",
    "    melkwargs={\n",
    "        \"n_fft\": 400,      # 25 ms window @16 kHz\n",
    "        \"hop_length\": 160, # 10 ms hop\n",
    "        \"n_mels\": 64\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35bd581",
   "metadata": {},
   "source": [
    "### New MFCCdataset class (like the AccentDataset class, but now for MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ebde6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        transform,\n",
    "        num_classes=5,\n",
    "        sample_rate=16000,\n",
    "        max_length=16000\n",
    "    ):\n",
    "        self.transform    = transform\n",
    "        self.num_classes  = num_classes\n",
    "        self.sample_rate  = sample_rate\n",
    "        self.max_length   = max_length\n",
    "        self.files        = []\n",
    "        for root, _, fns in os.walk(data_dir):\n",
    "            for fn in sorted(fns):\n",
    "                if not fn.lower().endswith('.wav'):\n",
    "                    continue\n",
    "                lbl = int(fn[0]) - 1\n",
    "                if 0 <= lbl < num_classes:\n",
    "                    self.files.append((os.path.join(root, fn), lbl))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.files[idx]\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        # resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            wav = T.Resample(sr, self.sample_rate)(wav)\n",
    "        wav = wav.mean(dim=0, keepdim=True)      # mono\n",
    "\n",
    "        # pad or truncate waveform → fixed max_length\n",
    "        if wav.size(1) < self.max_length:\n",
    "            pad = self.max_length - wav.size(1)\n",
    "            wav = nn.functional.pad(wav, (0, pad))\n",
    "        else:\n",
    "            wav = wav[:, :self.max_length]\n",
    "\n",
    "        # now transform to MFCC → all outputs share same T\n",
    "        mfcc = self.transform(wav)               # [1, n_mfcc, T_fixed]\n",
    "        # per–feature normalization\n",
    "        mfcc = (mfcc - mfcc.mean(dim=-1,keepdim=True)) / (\n",
    "               mfcc.std(dim=-1, keepdim=True) + 1e-5)\n",
    "\n",
    "        return mfcc.to(device), torch.tensor(label, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b0ac7",
   "metadata": {},
   "source": [
    "### MFCC Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "52ad51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mfcc_train_ds = MFCCDataset(\n",
    "    'Train',\n",
    "    transform=mfcc_transform,\n",
    "    num_classes=5,\n",
    "    max_length=16000\n",
    ")\n",
    "mfcc_test_ds = MFCCDataset(\n",
    "    'Test set',\n",
    "    transform=mfcc_transform,\n",
    "    num_classes=5,\n",
    "    max_length=16000\n",
    ")\n",
    "\n",
    "mfcc_train_loader = DataLoader(\n",
    "    mfcc_train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "mfcc_test_loader = DataLoader(\n",
    "    mfcc_test_ds,\n",
    "    batch_size=32\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500986f5",
   "metadata": {},
   "source": [
    "### 2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5fd0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MFCC_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=5, n_mfcc=40, hop_length=160, max_length=16000):\n",
    "        super().__init__()\n",
    "        # 2×2 pooling twice on both axes\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            # output shape: [B, 32, freq_dim=10, time_dim=25]\n",
    "        )\n",
    "        freq_dim = n_mfcc // 4             # 40//4=10\n",
    "        time_steps = math.ceil(max_length/hop_length) // 4  # 100//4=25\n",
    "        flat_size = 32 * freq_dim * time_steps  # 32*10*25=8000\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# re-instantiate your MFCC model\n",
    "mfcc_model = MFCC_CNN(num_classes=5).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab1f42",
   "metadata": {},
   "source": [
    "### Train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d8f9b3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MFCC] Epoch 01 | Train Acc: 0.2277 | Test Acc: 0.2081\n",
      "[MFCC] Epoch 02 | Train Acc: 0.2401 | Test Acc: 0.2114\n",
      "[MFCC] Epoch 03 | Train Acc: 0.2394 | Test Acc: 0.2081\n",
      "[MFCC] Epoch 04 | Train Acc: 0.2527 | Test Acc: 0.2047\n",
      "[MFCC] Epoch 05 | Train Acc: 0.2865 | Test Acc: 0.2047\n",
      "[MFCC] Epoch 06 | Train Acc: 0.3013 | Test Acc: 0.2114\n",
      "[MFCC] Epoch 07 | Train Acc: 0.3092 | Test Acc: 0.2215\n",
      "[MFCC] Epoch 08 | Train Acc: 0.2944 | Test Acc: 0.1980\n",
      "[MFCC] Epoch 09 | Train Acc: 0.3159 | Test Acc: 0.2114\n",
      "[MFCC] Epoch 10 | Train Acc: 0.3099 | Test Acc: 0.2215\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion    = nn.CrossEntropyLoss()\n",
    "mfcc_optimizer = optim.Adam(mfcc_model.parameters(), lr=1e-3)\n",
    "\n",
    "def run_mfcc_epoch(loader, train=True):\n",
    "    model = mfcc_model\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = correct = total = 0\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for X, y in loader:\n",
    "            logits = model(X)\n",
    "            loss   = criterion(logits, y)\n",
    "            if train:\n",
    "                mfcc_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                mfcc_optimizer.step()\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            correct    += (logits.argmax(1) == y).sum().item()\n",
    "            total      += X.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    tr_l, tr_a = run_mfcc_epoch(mfcc_train_loader, train=True)\n",
    "    te_l, te_a = run_mfcc_epoch(mfcc_test_loader,  train=False)\n",
    "    print(f\"[MFCC] Epoch {epoch:02d} | Train Acc: {tr_a:.4f} | Test Acc: {te_a:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2634daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: rebuild optimizer & scheduler for MFCC model\n",
    "mfcc_optimizer = optim.Adam(\n",
    "    mfcc_model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    mfcc_optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0edef804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sched] E01 | LR 1.0e-03 | Train Acc 0.320 | Test Acc 0.228\n",
      "[Sched] E02 | LR 1.0e-03 | Train Acc 0.358 | Test Acc 0.248\n",
      "[Sched] E03 | LR 1.0e-03 | Train Acc 0.340 | Test Acc 0.218\n",
      "[Sched] E04 | LR 1.0e-03 | Train Acc 0.373 | Test Acc 0.225\n",
      "[Sched] E05 | LR 5.0e-04 | Train Acc 0.399 | Test Acc 0.198\n",
      "[Sched] E06 | LR 5.0e-04 | Train Acc 0.399 | Test Acc 0.221\n",
      "[Sched] E07 | LR 5.0e-04 | Train Acc 0.436 | Test Acc 0.255\n",
      "[Sched] E08 | LR 5.0e-04 | Train Acc 0.425 | Test Acc 0.238\n",
      "[Sched] E09 | LR 5.0e-04 | Train Acc 0.440 | Test Acc 0.248\n",
      "[Sched] E10 | LR 2.5e-04 | Train Acc 0.462 | Test Acc 0.248\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: train for 10 epochs, stepping the LR scheduler each epoch\n",
    "for epoch in range(1, 11):\n",
    "    tr_l, tr_a = run_mfcc_epoch(mfcc_train_loader, train=True)\n",
    "    scheduler.step()   # decay LR if needed\n",
    "    te_l, te_a = run_mfcc_epoch(mfcc_test_loader, train=False)\n",
    "    print(f\"[Sched] E{epoch:02d} | LR {scheduler.get_last_lr()[0]:.1e} | \"\n",
    "          f\"Train Acc {tr_a:.3f} | Test Acc {te_a:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6aec6035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation results (Test Acc): {'do=0.0_bn=False': 0.26174496644295303, 'do=0.0_bn=True': 0.24161073825503357, 'do=0.3_bn=False': 0.2516778523489933, 'do=0.3_bn=True': 0.2348993288590604, 'do=0.5_bn=False': 0.22483221476510068, 'do=0.5_bn=True': 0.21140939597315436}\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: quick ablation over dropout rates and BN on/off\n",
    "from copy import deepcopy\n",
    "\n",
    "def ablate(dropout, use_bn):\n",
    "    m = deepcopy(mfcc_model)\n",
    "    # adjust dropout & batchnorm modules\n",
    "    for module in m.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.p = dropout\n",
    "        if isinstance(module, nn.BatchNorm2d) and not use_bn:\n",
    "            # replace BN with identity\n",
    "            setattr(module, \"forward\", lambda x: x)\n",
    "    opt = optim.Adam(m.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    for _ in range(5):\n",
    "        _ = run_mfcc_epoch(mfcc_train_loader, train=True)\n",
    "    _, acc = run_mfcc_epoch(mfcc_test_loader, train=False)\n",
    "    return acc\n",
    "\n",
    "results = {\n",
    "    f\"do={d}_bn={bn}\": ablate(d, bn)\n",
    "    for d in [0.0, 0.3, 0.5]\n",
    "    for bn in [False, True]\n",
    "}\n",
    "print(\"Ablation results (Test Acc):\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375990a",
   "metadata": {},
   "source": [
    "### Augmenting on previous MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8‑c: Build augmented MFCC train loader (replace original mfcc_train_loader)\n",
    "import random\n",
    "\n",
    "class AugmentedMFCCDataset(MFCCDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        # load fixed‑length waveform & label\n",
    "        wav, label = super(MFCCDataset, self).__getitem__(idx)  # [1, max_length]\n",
    "        # random time‑shift ±10%\n",
    "        max_shift = int(0.1 * self.max_length)\n",
    "        offset = random.randint(-max_shift, max_shift)\n",
    "        if offset > 0:\n",
    "            wav = torch.cat([wav[:, offset:], torch.zeros(1, offset, device=wav.device)], dim=1)\n",
    "        elif offset < 0:\n",
    "            wav = torch.cat([torch.zeros(1, -offset, device=wav.device), wav[:, :offset]], dim=1)\n",
    "        # additive Gaussian noise (SNR ~20dB)\n",
    "        noise = torch.randn_like(wav) * 0.05\n",
    "        wav = wav + noise\n",
    "        # recompute MFCC\n",
    "        mfcc = mfcc_transform(wav)\n",
    "        mfcc = (mfcc - mfcc.mean(dim=-1, keepdim=True)) / (mfcc.std(dim=-1, keepdim=True) + 1e-5)\n",
    "        return mfcc.to(device), label\n",
    "\n",
    "aug_train_ds = AugmentedMFCCDataset(\n",
    "    'Train',\n",
    "    transform=mfcc_transform,\n",
    "    num_classes=5,\n",
    "    max_length=16000\n",
    ")\n",
    "aug_train_loader = DataLoader(aug_train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "600a0661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sched] E01 | LR 2.5e-04 | Train Acc 0.597 | Test Acc 0.255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 12: train for 10 epochs, stepping the LR scheduler each epoch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     tr_l, tr_a \u001b[38;5;241m=\u001b[39m run_mfcc_epoch(mfcc_train_loader, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()   \u001b[38;5;66;03m# decay LR if needed\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     te_l, te_a \u001b[38;5;241m=\u001b[39m run_mfcc_epoch(mfcc_test_loader, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[80], line 9\u001b[0m, in \u001b[0;36mrun_mfcc_epoch\u001b[0;34m(loader, train)\u001b[0m\n\u001b[1;32m      7\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m=\u001b[39m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(train):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     10\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     11\u001b[0m         loss   \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[72], line 42\u001b[0m, in \u001b[0;36mMFCCDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m     wav \u001b[38;5;241m=\u001b[39m wav[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# now transform to MFCC → all outputs share same T\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(wav)               \u001b[38;5;66;03m# [1, n_mfcc, T_fixed]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# per–feature normalization\u001b[39;00m\n\u001b[1;32m     44\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m (mfcc \u001b[38;5;241m-\u001b[39m mfcc\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;241m/\u001b[39m (\n\u001b[1;32m     45\u001b[0m        mfcc\u001b[38;5;241m.\u001b[39mstd(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchaudio/transforms/_transforms.py:699\u001b[0m, in \u001b[0;36mMFCC.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m        Tensor: specgram_mel_db of size (..., ``n_mfcc``, time).\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m     mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMelSpectrogram(waveform)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_mels:\n\u001b[1;32m    701\u001b[0m         log_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchaudio/transforms/_transforms.py:619\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    612\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 619\u001b[0m     specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrogram(waveform)\n\u001b[1;32m    620\u001b[0m     mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_scale(specgram)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchaudio/transforms/_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mspectrogram(\n\u001b[1;32m    111\u001b[0m         waveform,\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow,\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_fft,\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhop_length,\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwin_length,\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower,\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized,\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter,\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_mode,\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monesided,\n\u001b[1;32m    122\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchaudio/functional/functional.py:126\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    123\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstft(\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mwaveform,\n\u001b[1;32m    128\u001b[0m     n_fft\u001b[38;5;241m=\u001b[39mn_fft,\n\u001b[1;32m    129\u001b[0m     hop_length\u001b[38;5;241m=\u001b[39mhop_length,\n\u001b[1;32m    130\u001b[0m     win_length\u001b[38;5;241m=\u001b[39mwin_length,\n\u001b[1;32m    131\u001b[0m     window\u001b[38;5;241m=\u001b[39mwindow,\n\u001b[1;32m    132\u001b[0m     center\u001b[38;5;241m=\u001b[39mcenter,\n\u001b[1;32m    133\u001b[0m     pad_mode\u001b[38;5;241m=\u001b[39mpad_mode,\n\u001b[1;32m    134\u001b[0m     normalized\u001b[38;5;241m=\u001b[39mframe_length_norm,\n\u001b[1;32m    135\u001b[0m     onesided\u001b[38;5;241m=\u001b[39monesided,\n\u001b[1;32m    136\u001b[0m     return_complex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# unpack batch\u001b[39;00m\n\u001b[1;32m    140\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m spec_f\u001b[38;5;241m.\u001b[39mreshape(shape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m spec_f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/functional.py:730\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex, align_to_window)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mstft(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    732\u001b[0m     n_fft,\n\u001b[1;32m    733\u001b[0m     hop_length,\n\u001b[1;32m    734\u001b[0m     win_length,\n\u001b[1;32m    735\u001b[0m     window,\n\u001b[1;32m    736\u001b[0m     normalized,\n\u001b[1;32m    737\u001b[0m     onesided,\n\u001b[1;32m    738\u001b[0m     return_complex,\n\u001b[1;32m    739\u001b[0m     align_to_window,\n\u001b[1;32m    740\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 12: train for 10 epochs, stepping the LR scheduler each epoch\n",
    "for epoch in range(1, 11):\n",
    "    tr_l, tr_a = run_mfcc_epoch(mfcc_train_loader, train=True)\n",
    "    scheduler.step()   # decay LR if needed\n",
    "    te_l, te_a = run_mfcc_epoch(mfcc_test_loader, train=False)\n",
    "    print(f\"[Sched] E{epoch:02d} | LR {scheduler.get_last_lr()[0]:.1e} | \"\n",
    "          f\"Train Acc {tr_a:.3f} | Test Acc {te_a:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "06be0167",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Subclasses of Dataset should implement __getitem__.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(mfcc_optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# train on augmented data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     tr_l, tr_a \u001b[38;5;241m=\u001b[39m run_mfcc_epoch(aug_train_loader, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# test on clean test set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[80], line 9\u001b[0m, in \u001b[0;36mrun_mfcc_epoch\u001b[0;34m(loader, train)\u001b[0m\n\u001b[1;32m      7\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m=\u001b[39m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(train):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     10\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     11\u001b[0m         loss   \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[84], line 6\u001b[0m, in \u001b[0;36mAugmentedMFCCDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# 1) Load the fixed‐length waveform & label\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     wav, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(MFCCDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(idx)  \u001b[38;5;66;03m# returns [1,max_len]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# 2) Random time‐shift ±10%\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     max_shift \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:59\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T_co:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubclasses of Dataset should implement __getitem__.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Subclasses of Dataset should implement __getitem__."
     ]
    }
   ],
   "source": [
    "# Cell 12‑b: Training loop using augmented data and scheduler\n",
    "\n",
    "# Rebuild optimizer and scheduler after kernel reset\n",
    "mfcc_optimizer = optim.Adam(mfcc_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(mfcc_optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    # train on augmented data\n",
    "    tr_l, tr_a = run_mfcc_epoch(aug_train_loader, train=True)\n",
    "    scheduler.step()\n",
    "    # test on clean test set\n",
    "    te_l, te_a = run_mfcc_epoch(mfcc_test_loader, train=False)\n",
    "    print(f\"[Aug+Sched] E{epoch:02d} | LR {scheduler.get_last_lr()[0]:.1e} | \"\n",
    "          f\"Train Acc {tr_a:.3f} | Test Acc {te_a:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b139dfd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
